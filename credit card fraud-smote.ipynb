{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "FILE_NAME = 'creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Label  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "full_data = pd.read_csv(FILE_NAME)\n",
    "\n",
    "#rename the 'Class' column\n",
    "full_data.rename(columns = {'Class': 'Label'}, inplace = True)\n",
    "\n",
    "#let's take a peek\n",
    "print full_data.shape\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consists of 284807 instances of data with 29 total features with value counts of \n",
      "0    284315\n",
      "1       492\n",
      "Name: Label, dtype: int64\n",
      "Where 0 indicates a legitimate transaction and 1 indicates fraud\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "full_data = shuffle(full_data)\n",
    "\n",
    "# Seperate target labels\n",
    "labels = full_data['Label']\n",
    "\n",
    "times = full_data['Time']\n",
    "features = full_data.drop(['Time', 'Label'], axis=1)\n",
    "\n",
    "# Get some specifics on our dataset\n",
    "print \"Data consists of {} instances of data with {} total features with value counts of \\n{}\".format(\n",
    "    features.shape[0], features.shape[1], labels.value_counts())\n",
    "print \"Where 0 indicates a legitimate transaction and 1 indicates fraud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the amount spent\n",
    "features['normAmount'] = StandardScaler().fit_transform(features['Amount'].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42914</th>\n",
       "      <td>1.328755</td>\n",
       "      <td>-0.667281</td>\n",
       "      <td>0.143056</td>\n",
       "      <td>-0.525633</td>\n",
       "      <td>-1.193573</td>\n",
       "      <td>-1.171937</td>\n",
       "      <td>-0.389395</td>\n",
       "      <td>-0.194001</td>\n",
       "      <td>-0.661954</td>\n",
       "      <td>0.734371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.515663</td>\n",
       "      <td>-0.384769</td>\n",
       "      <td>-0.798811</td>\n",
       "      <td>0.033199</td>\n",
       "      <td>0.379640</td>\n",
       "      <td>0.198983</td>\n",
       "      <td>1.042043</td>\n",
       "      <td>-0.085049</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>-0.165319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64702</th>\n",
       "      <td>-2.073365</td>\n",
       "      <td>1.212988</td>\n",
       "      <td>-0.210357</td>\n",
       "      <td>-3.817156</td>\n",
       "      <td>-0.342807</td>\n",
       "      <td>-1.086642</td>\n",
       "      <td>-0.385260</td>\n",
       "      <td>-0.907403</td>\n",
       "      <td>1.565272</td>\n",
       "      <td>-2.545380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710325</td>\n",
       "      <td>1.419176</td>\n",
       "      <td>-0.815893</td>\n",
       "      <td>-0.130419</td>\n",
       "      <td>-0.460201</td>\n",
       "      <td>0.480698</td>\n",
       "      <td>-1.173772</td>\n",
       "      <td>0.126028</td>\n",
       "      <td>-0.014738</td>\n",
       "      <td>-0.329041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23423</th>\n",
       "      <td>-8.913464</td>\n",
       "      <td>-7.608227</td>\n",
       "      <td>1.337346</td>\n",
       "      <td>-0.145191</td>\n",
       "      <td>1.546935</td>\n",
       "      <td>-0.219715</td>\n",
       "      <td>3.960317</td>\n",
       "      <td>-3.785540</td>\n",
       "      <td>3.959629</td>\n",
       "      <td>3.356782</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.157627</td>\n",
       "      <td>-2.924978</td>\n",
       "      <td>1.494810</td>\n",
       "      <td>2.426169</td>\n",
       "      <td>0.311150</td>\n",
       "      <td>1.370135</td>\n",
       "      <td>0.838952</td>\n",
       "      <td>-2.138323</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>1.026110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113824</th>\n",
       "      <td>1.290819</td>\n",
       "      <td>0.086102</td>\n",
       "      <td>-1.302560</td>\n",
       "      <td>-0.056117</td>\n",
       "      <td>2.253052</td>\n",
       "      <td>3.293045</td>\n",
       "      <td>-0.354302</td>\n",
       "      <td>0.763154</td>\n",
       "      <td>-0.153879</td>\n",
       "      <td>0.072484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009015</td>\n",
       "      <td>-0.022624</td>\n",
       "      <td>-0.178238</td>\n",
       "      <td>-0.139225</td>\n",
       "      <td>1.004248</td>\n",
       "      <td>0.846114</td>\n",
       "      <td>-0.313342</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>-0.317247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109444</th>\n",
       "      <td>-0.358580</td>\n",
       "      <td>0.966068</td>\n",
       "      <td>1.145574</td>\n",
       "      <td>-0.118194</td>\n",
       "      <td>0.114540</td>\n",
       "      <td>-0.506351</td>\n",
       "      <td>0.493812</td>\n",
       "      <td>0.182737</td>\n",
       "      <td>-0.399004</td>\n",
       "      <td>-0.295956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033848</td>\n",
       "      <td>-0.252584</td>\n",
       "      <td>-0.763710</td>\n",
       "      <td>-0.053691</td>\n",
       "      <td>-0.095451</td>\n",
       "      <td>-0.198796</td>\n",
       "      <td>0.080296</td>\n",
       "      <td>0.231897</td>\n",
       "      <td>0.081307</td>\n",
       "      <td>-0.346073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "42914   1.328755 -0.667281  0.143056 -0.525633 -1.193573 -1.171937 -0.389395   \n",
       "64702  -2.073365  1.212988 -0.210357 -3.817156 -0.342807 -1.086642 -0.385260   \n",
       "23423  -8.913464 -7.608227  1.337346 -0.145191  1.546935 -0.219715  3.960317   \n",
       "113824  1.290819  0.086102 -1.302560 -0.056117  2.253052  3.293045 -0.354302   \n",
       "109444 -0.358580  0.966068  1.145574 -0.118194  0.114540 -0.506351  0.493812   \n",
       "\n",
       "              V8        V9       V10     ...           V20       V21  \\\n",
       "42914  -0.194001 -0.661954  0.734371     ...     -0.515663 -0.384769   \n",
       "64702  -0.907403  1.565272 -2.545380     ...     -0.710325  1.419176   \n",
       "23423  -3.785540  3.959629  3.356782     ...     -7.157627 -2.924978   \n",
       "113824  0.763154 -0.153879  0.072484     ...     -0.009015 -0.022624   \n",
       "109444  0.182737 -0.399004 -0.295956     ...      0.033848 -0.252584   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "42914  -0.798811  0.033199  0.379640  0.198983  1.042043 -0.085049  0.007608   \n",
       "64702  -0.815893 -0.130419 -0.460201  0.480698 -1.173772  0.126028 -0.014738   \n",
       "23423   1.494810  2.426169  0.311150  1.370135  0.838952 -2.138323  0.008645   \n",
       "113824 -0.178238 -0.139225  1.004248  0.846114 -0.313342  0.015407  0.014375   \n",
       "109444 -0.763710 -0.053691 -0.095451 -0.198796  0.080296  0.231897  0.081307   \n",
       "\n",
       "        normAmount  \n",
       "42914    -0.165319  \n",
       "64702    -0.329041  \n",
       "23423     1.026110  \n",
       "113824   -0.317247  \n",
       "109444   -0.346073  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts = features['Amount']\n",
    "features = features.drop(['Amount'], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#?????????????????\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# implement synthetic minority oversampling technique for a more balanced dataset to feed our model\n",
    "oversampler = SMOTE(random_state=331)\n",
    "os_features, os_labels = oversampler.fit_sample(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training instances of data: 398041\n",
      "training instances of fraud 199140\n",
      "testing instances of data: 170589\n",
      "testing instances of fraud: 85175\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(os_features, os_labels, test_size = .3)\n",
    "\n",
    "# Let's get an idea of what our new oversampled data looks like\n",
    "\n",
    "print 'training instances of data:' , len(y_train) \n",
    "print 'training instances of fraud' , np.count_nonzero(y_train)\n",
    "print 'testing instances of data:' , len(y_test) \n",
    "print 'testing instances of fraud:' , np.count_nonzero(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Random Forest Classifier:\n",
      "[ 0.99984698  0.99977312  0.99988392] 0.999834673774\n",
      "For K-Nearest Neighbors Classifier:\n",
      "[ 0.99902831  0.99874278  0.998986  ] 0.998919028957\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "print \"For Random Forest Classifier:\"\n",
    "rfscores = cross_val_score(rf, os_features, os_labels, scoring = f1_scorer)\n",
    "print rfscores, rfscores.mean()\n",
    "\n",
    "print \"For K-Nearest Neighbors Classifier:\"\n",
    "knnscores = cross_val_score(knn, os_features, os_labels, scoring = f1_scorer)\n",
    "print knnscores, knnscores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for simple majority vote is  0.999135510488\n"
     ]
    }
   ],
   "source": [
    "#majority vote benchmark without oversampling\n",
    "majority_vote_predictions = np.zeros(features.shape[0])\n",
    "print \"f1 score for simple majority vote is \" , f1_score(labels, majority_vote_predictions, pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=entropy, max_features=21, score=0.999847 -   2.0s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=entropy, max_features=21 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=10, min_samples_split=6, criterion=entropy, max_features=21, score=0.999784 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=entropy, max_features=21 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=10, min_samples_split=6, criterion=entropy, max_features=21, score=0.999810 -   0.2s\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=13, score=0.999847 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=13, score=0.999836 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=13, score=0.999858 -   0.6s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=25, score=0.999583 -   4.2s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=25, score=0.999594 -   2.9s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=25, score=0.999657 -   3.0s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999858 -   2.4s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999847 -   2.1s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999863 -   2.0s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=5, score=0.999884 -   3.0s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=5, score=0.999831 -   3.1s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=5, score=0.999884 -   3.1s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=5, score=0.999868 -   1.1s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=5, score=0.999847 -   1.2s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=5, score=0.999873 -   1.4s\n",
      "[CV] n_estimators=55, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=55, min_samples_split=4, criterion=entropy, max_features=13, score=0.999852 -   1.3s\n",
      "[CV] n_estimators=55, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=55, min_samples_split=4, criterion=entropy, max_features=13, score=0.999858 -   1.3s\n",
      "[CV] n_estimators=55, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=55, min_samples_split=4, criterion=entropy, max_features=13, score=0.999863 -   1.3s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=21, score=0.999784 -   6.5s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=21, score=0.999726 -   2.9s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=21, score=0.999763 -   5.6s\n",
      "[CV] n_estimators=40, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=2, criterion=gini, max_features=1, score=0.999873 -   1.7s\n",
      "[CV] n_estimators=40, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=2, criterion=gini, max_features=1, score=0.999858 -   1.6s\n",
      "[CV] n_estimators=40, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=40, min_samples_split=2, criterion=gini, max_features=1, score=0.999900 -   1.7s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=5, score=0.999884 -  24.9s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=5, score=0.999826 -   3.3s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=5, score=0.999894 -   3.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999831 -   0.7s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999715 -   0.3s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999800 -   0.3s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=gini, max_features=21, score=0.999768 -   2.3s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=gini, max_features=21, score=0.999705 -   2.0s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=gini, max_features=21, score=0.999757 -   2.0s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1, score=0.999879 -   2.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1, score=0.999836 -   2.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=1, score=0.999873 -   2.7s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=9, score=0.999873 -   1.5s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=9, score=0.999842 -   1.6s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=9, score=0.999868 -   1.6s\n",
      "[CV] n_estimators=40, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=40, min_samples_split=2, criterion=gini, max_features=13, score=0.999826 -  21.8s\n",
      "[CV] n_estimators=40, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=40, min_samples_split=2, criterion=gini, max_features=13, score=0.999831 -   1.1s\n",
      "[CV] n_estimators=40, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=40, min_samples_split=2, criterion=gini, max_features=13, score=0.999858 -   1.1s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9, score=0.999868 -   0.9s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9, score=0.999831 -   1.0s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=9, score=0.999889 -   0.9s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=25, score=0.999826 -   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=25, score=0.999815 -   3.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=25, score=0.999784 -   2.3s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999842 -   1.6s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999852 -   1.7s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999836 -   1.6s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=gini, max_features=1, score=0.999889 -   3.0s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=gini, max_features=1, score=0.999858 -   3.3s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=gini, max_features=1, score=0.999873 -   2.8s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=13, score=0.999858 -   1.7s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=13, score=0.999847 -   1.7s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=13, score=0.999852 -   1.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=25, score=0.999573 -   2.5s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=25, score=0.999631 -   2.5s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=25, score=0.999662 -   2.7s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=1, score=0.999894 -   2.4s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=1, score=0.999863 -   2.6s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=1, score=0.999879 -   2.4s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=gini, max_features=9, score=0.999879 -   2.0s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=gini, max_features=9, score=0.999847 -   2.2s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=gini, max_features=9, score=0.999852 -   2.7s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=17, score=0.999831 -   1.6s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=17, score=0.999799 -  11.9s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=17, score=0.999815 -   1.6s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=13, score=0.999873 -   2.9s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=13, score=0.999831 -   3.1s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=13, score=0.999847 -   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 908.1min finished\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators' : np.arange(10, 110, 15),\n",
    "                'min_samples_split': np.arange(2, 8, 2),\n",
    "                'max_features': np.arange(1, 29, 4),\n",
    "                'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "rf_tune = RandomizedSearchCV(rf, rf_params, n_iter = 25, scoring = f1_scorer, verbose = 3)\n",
    "\n",
    "rf_tune = rf_tune.fit(os_features, os_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=70, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False) \n",
      "f1 score: 0.999878640823\n"
     ]
    }
   ],
   "source": [
    "print rf_tune.best_estimator_ , '\\nf1 score:' , rf_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.77979815,  -2.4083033 ,  -6.78178904, ..., -11.51279393,\n",
       "         -2.64945294,  -3.14909065],\n",
       "       [  4.64240508,  -4.99037778,  -5.86794173, ...,  -7.66904746,\n",
       "         -7.7372006 , -13.11827082],\n",
       "       [  1.08121135,  -1.18875625,   0.43640721, ...,   2.49536783,\n",
       "          0.5319671 ,   0.96720549],\n",
       "       ..., \n",
       "       [  4.33728493,  -3.8956539 ,  -4.93466896, ...,  -6.65599087,\n",
       "         -3.77057018,  -3.96300091],\n",
       "       [  5.17480293,  -4.51445392,  -0.27586251, ...,  -7.94539257,\n",
       "         -1.69056425,  -3.27894219],\n",
       "       [ -1.22037931,  -0.76978865,   1.29833454, ...,  -0.6222773 ,\n",
       "         -0.23289928,   0.35035864]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft = rf_tune.best_estimator_\n",
    "rft.fit_transform(X_train, y_train)\n",
    "\n",
    "rfu = RandomForestClassifier()\n",
    "rfu.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 testing score for tuned random forest is  0.999900474794\n",
      "f1 testing score for random forest is  0.999777551427\n"
     ]
    }
   ],
   "source": [
    "# Check performances of tuned and untuned models\n",
    "print \"f1 testing score for tuned random forest is \", f1_score(y_test, rft.predict(X_test), pos_label = 0)\n",
    "\n",
    "print \"f1 testing score for random forest is \" , f1_score(y_test, rfu.predict(X_test), pos_label = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops, looks like I could have chosen a better selection of hyper parameters for the randomized search cross validation optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85397,    17],\n",
       "       [    0, 85175]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rft.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now let's try a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a model\n",
    "\n",
    "# add regularization\n",
    "# needs more tuning\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim = X_train.shape[1], activation = 'tanh', init = 'lecun_uniform', W_regularizer = l2(.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(12, activation = 'tanh', init = 'lecun_uniform', W_regularizer = l2(.001)))\n",
    "model.add(Dense(4, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "sgd = SGD(lr = .15, momentum = .85, decay = .001)\n",
    "\n",
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 318432 samples, validate on 79609 samples\n",
      "Epoch 1/331\n",
      "2s - loss: 0.1508 - fmeasure: 0.9554 - val_loss: 0.0650 - val_fmeasure: 0.9734\n",
      "Epoch 2/331\n",
      "2s - loss: 0.0951 - fmeasure: 0.9721 - val_loss: 0.0520 - val_fmeasure: 0.9777\n",
      "Epoch 3/331\n",
      "2s - loss: 0.0797 - fmeasure: 0.9773 - val_loss: 0.0446 - val_fmeasure: 0.9830\n",
      "Epoch 4/331\n",
      "2s - loss: 0.0721 - fmeasure: 0.9802 - val_loss: 0.0386 - val_fmeasure: 0.9865\n",
      "Epoch 5/331\n",
      "2s - loss: 0.0663 - fmeasure: 0.9826 - val_loss: 0.0383 - val_fmeasure: 0.9867\n",
      "Epoch 6/331\n",
      "2s - loss: 0.0620 - fmeasure: 0.9841 - val_loss: 0.0338 - val_fmeasure: 0.9895\n",
      "Epoch 7/331\n",
      "2s - loss: 0.0596 - fmeasure: 0.9852 - val_loss: 0.0329 - val_fmeasure: 0.9883\n",
      "Epoch 8/331\n",
      "2s - loss: 0.0578 - fmeasure: 0.9861 - val_loss: 0.0295 - val_fmeasure: 0.9904\n",
      "Epoch 9/331\n",
      "2s - loss: 0.0551 - fmeasure: 0.9873 - val_loss: 0.0283 - val_fmeasure: 0.9902\n",
      "Epoch 10/331\n",
      "2s - loss: 0.0537 - fmeasure: 0.9877 - val_loss: 0.0272 - val_fmeasure: 0.9911\n",
      "Epoch 11/331\n",
      "2s - loss: 0.0523 - fmeasure: 0.9884 - val_loss: 0.0263 - val_fmeasure: 0.9918\n",
      "Epoch 12/331\n",
      "2s - loss: 0.0516 - fmeasure: 0.9886 - val_loss: 0.0254 - val_fmeasure: 0.9919\n",
      "Epoch 13/331\n",
      "2s - loss: 0.0507 - fmeasure: 0.9890 - val_loss: 0.0293 - val_fmeasure: 0.9896\n",
      "Epoch 14/331\n",
      "3s - loss: 0.0501 - fmeasure: 0.9892 - val_loss: 0.0264 - val_fmeasure: 0.9917\n",
      "Epoch 15/331\n",
      "3s - loss: 0.0490 - fmeasure: 0.9895 - val_loss: 0.0256 - val_fmeasure: 0.9924\n",
      "Epoch 16/331\n",
      "2s - loss: 0.0488 - fmeasure: 0.9899 - val_loss: 0.0267 - val_fmeasure: 0.9916\n",
      "Epoch 17/331\n",
      "2s - loss: 0.0479 - fmeasure: 0.9899 - val_loss: 0.0245 - val_fmeasure: 0.9919\n",
      "Epoch 18/331\n",
      "2s - loss: 0.0474 - fmeasure: 0.9902 - val_loss: 0.0229 - val_fmeasure: 0.9936\n",
      "Epoch 19/331\n",
      "2s - loss: 0.0473 - fmeasure: 0.9901 - val_loss: 0.0239 - val_fmeasure: 0.9923\n",
      "Epoch 20/331\n",
      "2s - loss: 0.0469 - fmeasure: 0.9903 - val_loss: 0.0242 - val_fmeasure: 0.9923\n",
      "Epoch 21/331\n",
      "2s - loss: 0.0461 - fmeasure: 0.9908 - val_loss: 0.0236 - val_fmeasure: 0.9927\n",
      "Epoch 22/331\n",
      "2s - loss: 0.0459 - fmeasure: 0.9907 - val_loss: 0.0225 - val_fmeasure: 0.9924\n",
      "Epoch 23/331\n",
      "2s - loss: 0.0459 - fmeasure: 0.9906 - val_loss: 0.0225 - val_fmeasure: 0.9933\n",
      "Epoch 24/331\n",
      "2s - loss: 0.0452 - fmeasure: 0.9909 - val_loss: 0.0233 - val_fmeasure: 0.9924\n",
      "Epoch 25/331\n",
      "2s - loss: 0.0450 - fmeasure: 0.9910 - val_loss: 0.0232 - val_fmeasure: 0.9918\n",
      "Epoch 26/331\n",
      "2s - loss: 0.0447 - fmeasure: 0.9911 - val_loss: 0.0224 - val_fmeasure: 0.9935\n",
      "Epoch 27/331\n",
      "2s - loss: 0.0443 - fmeasure: 0.9912 - val_loss: 0.0209 - val_fmeasure: 0.9937\n",
      "Epoch 28/331\n",
      "2s - loss: 0.0443 - fmeasure: 0.9913 - val_loss: 0.0212 - val_fmeasure: 0.9936\n",
      "Epoch 29/331\n",
      "2s - loss: 0.0439 - fmeasure: 0.9913 - val_loss: 0.0213 - val_fmeasure: 0.9936\n",
      "Epoch 30/331\n",
      "2s - loss: 0.0438 - fmeasure: 0.9913 - val_loss: 0.0212 - val_fmeasure: 0.9931\n",
      "Epoch 31/331\n",
      "2s - loss: 0.0437 - fmeasure: 0.9915 - val_loss: 0.0212 - val_fmeasure: 0.9937\n",
      "Epoch 32/331\n",
      "2s - loss: 0.0432 - fmeasure: 0.9916 - val_loss: 0.0204 - val_fmeasure: 0.9944\n",
      "Epoch 33/331\n",
      "2s - loss: 0.0433 - fmeasure: 0.9915 - val_loss: 0.0210 - val_fmeasure: 0.9938\n",
      "Epoch 34/331\n",
      "3s - loss: 0.0428 - fmeasure: 0.9918 - val_loss: 0.0210 - val_fmeasure: 0.9940\n",
      "Epoch 35/331\n",
      "2s - loss: 0.0431 - fmeasure: 0.9914 - val_loss: 0.0208 - val_fmeasure: 0.9933\n",
      "Epoch 36/331\n",
      "2s - loss: 0.0423 - fmeasure: 0.9921 - val_loss: 0.0200 - val_fmeasure: 0.9943\n",
      "Epoch 37/331\n",
      "2s - loss: 0.0428 - fmeasure: 0.9917 - val_loss: 0.0201 - val_fmeasure: 0.9941\n",
      "Epoch 38/331\n",
      "2s - loss: 0.0424 - fmeasure: 0.9919 - val_loss: 0.0201 - val_fmeasure: 0.9939\n",
      "Epoch 39/331\n",
      "2s - loss: 0.0422 - fmeasure: 0.9919 - val_loss: 0.0205 - val_fmeasure: 0.9934\n",
      "Epoch 40/331\n",
      "2s - loss: 0.0420 - fmeasure: 0.9921 - val_loss: 0.0210 - val_fmeasure: 0.9941\n",
      "Epoch 41/331\n",
      "2s - loss: 0.0418 - fmeasure: 0.9922 - val_loss: 0.0203 - val_fmeasure: 0.9942\n",
      "Epoch 42/331\n",
      "2s - loss: 0.0420 - fmeasure: 0.9920 - val_loss: 0.0197 - val_fmeasure: 0.9944\n",
      "Epoch 43/331\n",
      "2s - loss: 0.0414 - fmeasure: 0.9923 - val_loss: 0.0198 - val_fmeasure: 0.9942\n",
      "Epoch 44/331\n",
      "2s - loss: 0.0417 - fmeasure: 0.9921 - val_loss: 0.0190 - val_fmeasure: 0.9945\n",
      "Epoch 45/331\n",
      "2s - loss: 0.0418 - fmeasure: 0.9920 - val_loss: 0.0199 - val_fmeasure: 0.9942\n",
      "Epoch 46/331\n",
      "2s - loss: 0.0416 - fmeasure: 0.9922 - val_loss: 0.0205 - val_fmeasure: 0.9938\n",
      "Epoch 47/331\n",
      "2s - loss: 0.0413 - fmeasure: 0.9922 - val_loss: 0.0197 - val_fmeasure: 0.9942\n",
      "Epoch 48/331\n",
      "2s - loss: 0.0417 - fmeasure: 0.9920 - val_loss: 0.0191 - val_fmeasure: 0.9945\n",
      "Epoch 49/331\n",
      "2s - loss: 0.0413 - fmeasure: 0.9924 - val_loss: 0.0200 - val_fmeasure: 0.9941\n",
      "Epoch 50/331\n",
      "2s - loss: 0.0409 - fmeasure: 0.9924 - val_loss: 0.0199 - val_fmeasure: 0.9939\n",
      "Epoch 51/331\n",
      "3s - loss: 0.0411 - fmeasure: 0.9923 - val_loss: 0.0204 - val_fmeasure: 0.9941\n",
      "Epoch 52/331\n",
      "2s - loss: 0.0408 - fmeasure: 0.9926 - val_loss: 0.0189 - val_fmeasure: 0.9948\n",
      "Epoch 53/331\n",
      "2s - loss: 0.0408 - fmeasure: 0.9926 - val_loss: 0.0201 - val_fmeasure: 0.9942\n",
      "Epoch 54/331\n",
      "2s - loss: 0.0406 - fmeasure: 0.9926 - val_loss: 0.0189 - val_fmeasure: 0.9946\n",
      "Epoch 55/331\n",
      "2s - loss: 0.0409 - fmeasure: 0.9924 - val_loss: 0.0192 - val_fmeasure: 0.9946\n",
      "Epoch 56/331\n",
      "2s - loss: 0.0406 - fmeasure: 0.9924 - val_loss: 0.0203 - val_fmeasure: 0.9940\n",
      "Epoch 57/331\n",
      "2s - loss: 0.0407 - fmeasure: 0.9925 - val_loss: 0.0193 - val_fmeasure: 0.9941\n",
      "Epoch 58/331\n",
      "2s - loss: 0.0405 - fmeasure: 0.9925 - val_loss: 0.0189 - val_fmeasure: 0.9946\n",
      "Epoch 59/331\n",
      "2s - loss: 0.0405 - fmeasure: 0.9925 - val_loss: 0.0190 - val_fmeasure: 0.9941\n",
      "Epoch 60/331\n",
      "2s - loss: 0.0405 - fmeasure: 0.9925 - val_loss: 0.0188 - val_fmeasure: 0.9945\n",
      "Epoch 61/331\n",
      "2s - loss: 0.0402 - fmeasure: 0.9928 - val_loss: 0.0183 - val_fmeasure: 0.9949\n",
      "Epoch 62/331\n",
      "2s - loss: 0.0402 - fmeasure: 0.9927 - val_loss: 0.0185 - val_fmeasure: 0.9946\n",
      "Epoch 63/331\n",
      "2s - loss: 0.0401 - fmeasure: 0.9927 - val_loss: 0.0182 - val_fmeasure: 0.9946\n",
      "Epoch 64/331\n",
      "2s - loss: 0.0404 - fmeasure: 0.9927 - val_loss: 0.0191 - val_fmeasure: 0.9945\n",
      "Epoch 65/331\n",
      "2s - loss: 0.0399 - fmeasure: 0.9929 - val_loss: 0.0184 - val_fmeasure: 0.9950\n",
      "Epoch 66/331\n",
      "2s - loss: 0.0401 - fmeasure: 0.9928 - val_loss: 0.0188 - val_fmeasure: 0.9945\n",
      "Epoch 67/331\n",
      "2s - loss: 0.0402 - fmeasure: 0.9928 - val_loss: 0.0186 - val_fmeasure: 0.9945\n",
      "Epoch 68/331\n",
      "2s - loss: 0.0398 - fmeasure: 0.9929 - val_loss: 0.0182 - val_fmeasure: 0.9949\n",
      "Epoch 69/331\n",
      "2s - loss: 0.0400 - fmeasure: 0.9929 - val_loss: 0.0188 - val_fmeasure: 0.9946\n",
      "Epoch 70/331\n",
      "2s - loss: 0.0400 - fmeasure: 0.9929 - val_loss: 0.0182 - val_fmeasure: 0.9948\n",
      "Epoch 71/331\n",
      "2s - loss: 0.0397 - fmeasure: 0.9928 - val_loss: 0.0180 - val_fmeasure: 0.9950\n",
      "Epoch 72/331\n",
      "2s - loss: 0.0396 - fmeasure: 0.9928 - val_loss: 0.0181 - val_fmeasure: 0.9948\n",
      "Epoch 73/331\n",
      "2s - loss: 0.0397 - fmeasure: 0.9929 - val_loss: 0.0184 - val_fmeasure: 0.9944\n",
      "Epoch 74/331\n",
      "2s - loss: 0.0392 - fmeasure: 0.9929 - val_loss: 0.0189 - val_fmeasure: 0.9945\n",
      "Epoch 75/331\n",
      "2s - loss: 0.0395 - fmeasure: 0.9928 - val_loss: 0.0182 - val_fmeasure: 0.9946\n",
      "Epoch 76/331\n",
      "2s - loss: 0.0393 - fmeasure: 0.9929 - val_loss: 0.0180 - val_fmeasure: 0.9950\n",
      "Epoch 77/331\n",
      "2s - loss: 0.0394 - fmeasure: 0.9929 - val_loss: 0.0175 - val_fmeasure: 0.9949\n",
      "Epoch 78/331\n",
      "2s - loss: 0.0393 - fmeasure: 0.9931 - val_loss: 0.0179 - val_fmeasure: 0.9948\n",
      "Epoch 79/331\n",
      "2s - loss: 0.0392 - fmeasure: 0.9931 - val_loss: 0.0186 - val_fmeasure: 0.9947\n",
      "Epoch 80/331\n",
      "2s - loss: 0.0390 - fmeasure: 0.9930 - val_loss: 0.0179 - val_fmeasure: 0.9949\n",
      "Epoch 81/331\n",
      "2s - loss: 0.0391 - fmeasure: 0.9930 - val_loss: 0.0184 - val_fmeasure: 0.9947\n",
      "Epoch 82/331\n",
      "2s - loss: 0.0394 - fmeasure: 0.9928 - val_loss: 0.0185 - val_fmeasure: 0.9947\n",
      "Epoch 83/331\n",
      "2s - loss: 0.0393 - fmeasure: 0.9929 - val_loss: 0.0178 - val_fmeasure: 0.9950\n",
      "Epoch 84/331\n",
      "2s - loss: 0.0392 - fmeasure: 0.9929 - val_loss: 0.0183 - val_fmeasure: 0.9944\n",
      "Epoch 85/331\n",
      "2s - loss: 0.0392 - fmeasure: 0.9930 - val_loss: 0.0179 - val_fmeasure: 0.9949\n",
      "Epoch 86/331\n",
      "2s - loss: 0.0389 - fmeasure: 0.9932 - val_loss: 0.0180 - val_fmeasure: 0.9949\n",
      "Epoch 87/331\n",
      "2s - loss: 0.0390 - fmeasure: 0.9929 - val_loss: 0.0189 - val_fmeasure: 0.9943\n",
      "Epoch 88/331\n",
      "2s - loss: 0.0391 - fmeasure: 0.9930 - val_loss: 0.0185 - val_fmeasure: 0.9946\n",
      "Epoch 89/331\n",
      "2s - loss: 0.0388 - fmeasure: 0.9931 - val_loss: 0.0186 - val_fmeasure: 0.9949\n",
      "Epoch 90/331\n",
      "2s - loss: 0.0387 - fmeasure: 0.9933 - val_loss: 0.0179 - val_fmeasure: 0.9950\n",
      "Epoch 91/331\n",
      "2s - loss: 0.0388 - fmeasure: 0.9931 - val_loss: 0.0178 - val_fmeasure: 0.9952\n",
      "Epoch 92/331\n",
      "2s - loss: 0.0387 - fmeasure: 0.9932 - val_loss: 0.0179 - val_fmeasure: 0.9951\n",
      "Epoch 93/331\n",
      "2s - loss: 0.0388 - fmeasure: 0.9931 - val_loss: 0.0175 - val_fmeasure: 0.9952\n",
      "Epoch 94/331\n",
      "2s - loss: 0.0386 - fmeasure: 0.9932 - val_loss: 0.0173 - val_fmeasure: 0.9950\n",
      "Epoch 95/331\n",
      "2s - loss: 0.0389 - fmeasure: 0.9930 - val_loss: 0.0183 - val_fmeasure: 0.9949\n",
      "Epoch 96/331\n",
      "2s - loss: 0.0388 - fmeasure: 0.9932 - val_loss: 0.0181 - val_fmeasure: 0.9949\n",
      "Epoch 97/331\n",
      "2s - loss: 0.0389 - fmeasure: 0.9932 - val_loss: 0.0181 - val_fmeasure: 0.9946\n",
      "Epoch 98/331\n",
      "2s - loss: 0.0385 - fmeasure: 0.9931 - val_loss: 0.0174 - val_fmeasure: 0.9951\n",
      "Epoch 99/331\n",
      "2s - loss: 0.0386 - fmeasure: 0.9931 - val_loss: 0.0178 - val_fmeasure: 0.9947\n",
      "Epoch 100/331\n",
      "2s - loss: 0.0384 - fmeasure: 0.9933 - val_loss: 0.0174 - val_fmeasure: 0.9951\n",
      "Epoch 101/331\n",
      "2s - loss: 0.0385 - fmeasure: 0.9933 - val_loss: 0.0171 - val_fmeasure: 0.9952\n",
      "Epoch 102/331\n",
      "2s - loss: 0.0386 - fmeasure: 0.9931 - val_loss: 0.0180 - val_fmeasure: 0.9947\n",
      "Epoch 103/331\n",
      "2s - loss: 0.0386 - fmeasure: 0.9931 - val_loss: 0.0176 - val_fmeasure: 0.9949\n",
      "Epoch 104/331\n",
      "2s - loss: 0.0384 - fmeasure: 0.9932 - val_loss: 0.0174 - val_fmeasure: 0.9949\n",
      "Epoch 105/331\n",
      "2s - loss: 0.0385 - fmeasure: 0.9932 - val_loss: 0.0172 - val_fmeasure: 0.9951\n",
      "Epoch 106/331\n",
      "2s - loss: 0.0383 - fmeasure: 0.9932 - val_loss: 0.0174 - val_fmeasure: 0.9952\n",
      "Epoch 107/331\n",
      "2s - loss: 0.0384 - fmeasure: 0.9933 - val_loss: 0.0176 - val_fmeasure: 0.9949\n",
      "Epoch 108/331\n",
      "3s - loss: 0.0383 - fmeasure: 0.9934 - val_loss: 0.0171 - val_fmeasure: 0.9955\n",
      "Epoch 109/331\n",
      "3s - loss: 0.0383 - fmeasure: 0.9933 - val_loss: 0.0176 - val_fmeasure: 0.9950\n",
      "Epoch 110/331\n",
      "2s - loss: 0.0381 - fmeasure: 0.9934 - val_loss: 0.0179 - val_fmeasure: 0.9950\n",
      "Epoch 111/331\n",
      "2s - loss: 0.0382 - fmeasure: 0.9933 - val_loss: 0.0172 - val_fmeasure: 0.9950\n",
      "Epoch 112/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9934 - val_loss: 0.0172 - val_fmeasure: 0.9951\n",
      "Epoch 113/331\n",
      "2s - loss: 0.0384 - fmeasure: 0.9931 - val_loss: 0.0170 - val_fmeasure: 0.9954\n",
      "Epoch 114/331\n",
      "2s - loss: 0.0382 - fmeasure: 0.9934 - val_loss: 0.0174 - val_fmeasure: 0.9951\n",
      "Epoch 115/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9936 - val_loss: 0.0172 - val_fmeasure: 0.9952\n",
      "Epoch 116/331\n",
      "2s - loss: 0.0381 - fmeasure: 0.9935 - val_loss: 0.0170 - val_fmeasure: 0.9953\n",
      "Epoch 117/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9935 - val_loss: 0.0171 - val_fmeasure: 0.9953\n",
      "Epoch 118/331\n",
      "2s - loss: 0.0381 - fmeasure: 0.9932 - val_loss: 0.0181 - val_fmeasure: 0.9949\n",
      "Epoch 119/331\n",
      "2s - loss: 0.0381 - fmeasure: 0.9933 - val_loss: 0.0170 - val_fmeasure: 0.9953\n",
      "Epoch 120/331\n",
      "2s - loss: 0.0380 - fmeasure: 0.9932 - val_loss: 0.0180 - val_fmeasure: 0.9946\n",
      "Epoch 121/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9934 - val_loss: 0.0178 - val_fmeasure: 0.9950\n",
      "Epoch 122/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9935 - val_loss: 0.0183 - val_fmeasure: 0.9946\n",
      "Epoch 123/331\n",
      "2s - loss: 0.0379 - fmeasure: 0.9933 - val_loss: 0.0172 - val_fmeasure: 0.9954\n",
      "Epoch 124/331\n",
      "2s - loss: 0.0380 - fmeasure: 0.9935 - val_loss: 0.0173 - val_fmeasure: 0.9950\n",
      "Epoch 125/331\n",
      "2s - loss: 0.0382 - fmeasure: 0.9930 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 126/331\n",
      "2s - loss: 0.0379 - fmeasure: 0.9933 - val_loss: 0.0176 - val_fmeasure: 0.9949\n",
      "Epoch 127/331\n",
      "3s - loss: 0.0382 - fmeasure: 0.9932 - val_loss: 0.0171 - val_fmeasure: 0.9952\n",
      "Epoch 128/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9935 - val_loss: 0.0172 - val_fmeasure: 0.9951\n",
      "Epoch 129/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9933 - val_loss: 0.0171 - val_fmeasure: 0.9952\n",
      "Epoch 130/331\n",
      "2s - loss: 0.0375 - fmeasure: 0.9936 - val_loss: 0.0170 - val_fmeasure: 0.9952\n",
      "Epoch 131/331\n",
      "2s - loss: 0.0377 - fmeasure: 0.9934 - val_loss: 0.0177 - val_fmeasure: 0.9950\n",
      "Epoch 132/331\n",
      "2s - loss: 0.0377 - fmeasure: 0.9935 - val_loss: 0.0173 - val_fmeasure: 0.9950\n",
      "Epoch 133/331\n",
      "2s - loss: 0.0378 - fmeasure: 0.9934 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 134/331\n",
      "2s - loss: 0.0377 - fmeasure: 0.9933 - val_loss: 0.0174 - val_fmeasure: 0.9948\n",
      "Epoch 135/331\n",
      "2s - loss: 0.0375 - fmeasure: 0.9935 - val_loss: 0.0168 - val_fmeasure: 0.9954\n",
      "Epoch 136/331\n",
      "2s - loss: 0.0376 - fmeasure: 0.9933 - val_loss: 0.0174 - val_fmeasure: 0.9952\n",
      "Epoch 137/331\n",
      "2s - loss: 0.0374 - fmeasure: 0.9936 - val_loss: 0.0174 - val_fmeasure: 0.9950\n",
      "Epoch 138/331\n",
      "3s - loss: 0.0378 - fmeasure: 0.9934 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 139/331\n",
      "3s - loss: 0.0374 - fmeasure: 0.9935 - val_loss: 0.0173 - val_fmeasure: 0.9952\n",
      "Epoch 140/331\n",
      "2s - loss: 0.0376 - fmeasure: 0.9934 - val_loss: 0.0170 - val_fmeasure: 0.9955\n",
      "Epoch 141/331\n",
      "2s - loss: 0.0375 - fmeasure: 0.9934 - val_loss: 0.0171 - val_fmeasure: 0.9951\n",
      "Epoch 142/331\n",
      "2s - loss: 0.0377 - fmeasure: 0.9934 - val_loss: 0.0170 - val_fmeasure: 0.9950\n",
      "Epoch 143/331\n",
      "2s - loss: 0.0377 - fmeasure: 0.9935 - val_loss: 0.0175 - val_fmeasure: 0.9950\n",
      "Epoch 144/331\n",
      "2s - loss: 0.0375 - fmeasure: 0.9935 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 145/331\n",
      "2s - loss: 0.0376 - fmeasure: 0.9933 - val_loss: 0.0172 - val_fmeasure: 0.9952\n",
      "Epoch 146/331\n",
      "2s - loss: 0.0376 - fmeasure: 0.9935 - val_loss: 0.0172 - val_fmeasure: 0.9951\n",
      "Epoch 147/331\n",
      "2s - loss: 0.0376 - fmeasure: 0.9935 - val_loss: 0.0169 - val_fmeasure: 0.9952\n",
      "Epoch 148/331\n",
      "2s - loss: 0.0374 - fmeasure: 0.9935 - val_loss: 0.0170 - val_fmeasure: 0.9955\n",
      "Epoch 149/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9937 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 150/331\n",
      "2s - loss: 0.0373 - fmeasure: 0.9935 - val_loss: 0.0174 - val_fmeasure: 0.9949\n",
      "Epoch 151/331\n",
      "2s - loss: 0.0375 - fmeasure: 0.9937 - val_loss: 0.0167 - val_fmeasure: 0.9954\n",
      "Epoch 152/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9934 - val_loss: 0.0168 - val_fmeasure: 0.9952\n",
      "Epoch 153/331\n",
      "2s - loss: 0.0374 - fmeasure: 0.9935 - val_loss: 0.0171 - val_fmeasure: 0.9952\n",
      "Epoch 154/331\n",
      "2s - loss: 0.0374 - fmeasure: 0.9935 - val_loss: 0.0166 - val_fmeasure: 0.9955\n",
      "Epoch 155/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9936 - val_loss: 0.0171 - val_fmeasure: 0.9950\n",
      "Epoch 156/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9936 - val_loss: 0.0169 - val_fmeasure: 0.9952\n",
      "Epoch 157/331\n",
      "2s - loss: 0.0373 - fmeasure: 0.9935 - val_loss: 0.0166 - val_fmeasure: 0.9953\n",
      "Epoch 158/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9935 - val_loss: 0.0172 - val_fmeasure: 0.9952\n",
      "Epoch 159/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9936 - val_loss: 0.0177 - val_fmeasure: 0.9950\n",
      "Epoch 160/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9936 - val_loss: 0.0169 - val_fmeasure: 0.9951\n",
      "Epoch 161/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9936 - val_loss: 0.0165 - val_fmeasure: 0.9956\n",
      "Epoch 162/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9937 - val_loss: 0.0172 - val_fmeasure: 0.9951\n",
      "Epoch 163/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9935 - val_loss: 0.0170 - val_fmeasure: 0.9952\n",
      "Epoch 164/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9938 - val_loss: 0.0168 - val_fmeasure: 0.9952\n",
      "Epoch 165/331\n",
      "2s - loss: 0.0370 - fmeasure: 0.9935 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 166/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9935 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 167/331\n",
      "3s - loss: 0.0369 - fmeasure: 0.9934 - val_loss: 0.0169 - val_fmeasure: 0.9952\n",
      "Epoch 168/331\n",
      "2s - loss: 0.0370 - fmeasure: 0.9936 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 169/331\n",
      "2s - loss: 0.0372 - fmeasure: 0.9936 - val_loss: 0.0170 - val_fmeasure: 0.9952\n",
      "Epoch 170/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9934 - val_loss: 0.0163 - val_fmeasure: 0.9956\n",
      "Epoch 171/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9935 - val_loss: 0.0171 - val_fmeasure: 0.9952\n",
      "Epoch 172/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9935 - val_loss: 0.0175 - val_fmeasure: 0.9951\n",
      "Epoch 173/331\n",
      "2s - loss: 0.0374 - fmeasure: 0.9934 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 174/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9937 - val_loss: 0.0167 - val_fmeasure: 0.9952\n",
      "Epoch 175/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9936 - val_loss: 0.0171 - val_fmeasure: 0.9952\n",
      "Epoch 176/331\n",
      "2s - loss: 0.0373 - fmeasure: 0.9935 - val_loss: 0.0168 - val_fmeasure: 0.9952\n",
      "Epoch 177/331\n",
      "2s - loss: 0.0373 - fmeasure: 0.9935 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 178/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9935 - val_loss: 0.0169 - val_fmeasure: 0.9952\n",
      "Epoch 179/331\n",
      "2s - loss: 0.0370 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9955\n",
      "Epoch 180/331\n",
      "2s - loss: 0.0368 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9954\n",
      "Epoch 181/331\n",
      "3s - loss: 0.0370 - fmeasure: 0.9936 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 182/331\n",
      "3s - loss: 0.0371 - fmeasure: 0.9935 - val_loss: 0.0170 - val_fmeasure: 0.9956\n",
      "Epoch 183/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9937 - val_loss: 0.0167 - val_fmeasure: 0.9954\n",
      "Epoch 184/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9937 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 185/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9936 - val_loss: 0.0170 - val_fmeasure: 0.9953\n",
      "Epoch 186/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9936 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 187/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9937 - val_loss: 0.0164 - val_fmeasure: 0.9957\n",
      "Epoch 188/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9935 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 189/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9935 - val_loss: 0.0170 - val_fmeasure: 0.9952\n",
      "Epoch 190/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9955\n",
      "Epoch 191/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9938 - val_loss: 0.0169 - val_fmeasure: 0.9951\n",
      "Epoch 192/331\n",
      "2s - loss: 0.0368 - fmeasure: 0.9938 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 193/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9935 - val_loss: 0.0168 - val_fmeasure: 0.9952\n",
      "Epoch 194/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9937 - val_loss: 0.0178 - val_fmeasure: 0.9948\n",
      "Epoch 195/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9936 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 196/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9952\n",
      "Epoch 197/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9937 - val_loss: 0.0166 - val_fmeasure: 0.9951\n",
      "Epoch 198/331\n",
      "2s - loss: 0.0368 - fmeasure: 0.9936 - val_loss: 0.0165 - val_fmeasure: 0.9954\n",
      "Epoch 199/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9937 - val_loss: 0.0164 - val_fmeasure: 0.9953\n",
      "Epoch 200/331\n",
      "2s - loss: 0.0370 - fmeasure: 0.9934 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 201/331\n",
      "2s - loss: 0.0369 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 202/331\n",
      "2s - loss: 0.0371 - fmeasure: 0.9934 - val_loss: 0.0168 - val_fmeasure: 0.9951\n",
      "Epoch 203/331\n",
      "2s - loss: 0.0370 - fmeasure: 0.9934 - val_loss: 0.0170 - val_fmeasure: 0.9952\n",
      "Epoch 204/331\n",
      "3s - loss: 0.0365 - fmeasure: 0.9938 - val_loss: 0.0170 - val_fmeasure: 0.9951\n",
      "Epoch 205/331\n",
      "2s - loss: 0.0368 - fmeasure: 0.9937 - val_loss: 0.0169 - val_fmeasure: 0.9951\n",
      "Epoch 206/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9954\n",
      "Epoch 207/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9954\n",
      "Epoch 208/331\n",
      "3s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 209/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9936 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 210/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9937 - val_loss: 0.0169 - val_fmeasure: 0.9952\n",
      "Epoch 211/331\n",
      "2s - loss: 0.0368 - fmeasure: 0.9935 - val_loss: 0.0171 - val_fmeasure: 0.9954\n",
      "Epoch 212/331\n",
      "3s - loss: 0.0367 - fmeasure: 0.9938 - val_loss: 0.0167 - val_fmeasure: 0.9952\n",
      "Epoch 213/331\n",
      "3s - loss: 0.0366 - fmeasure: 0.9938 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 214/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9937 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 215/331\n",
      "2s - loss: 0.0368 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 216/331\n",
      "2s - loss: 0.0370 - fmeasure: 0.9936 - val_loss: 0.0166 - val_fmeasure: 0.9953\n",
      "Epoch 217/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0163 - val_fmeasure: 0.9953\n",
      "Epoch 218/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9951\n",
      "Epoch 219/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0171 - val_fmeasure: 0.9950\n",
      "Epoch 220/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9937 - val_loss: 0.0172 - val_fmeasure: 0.9949\n",
      "Epoch 221/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9938 - val_loss: 0.0171 - val_fmeasure: 0.9953\n",
      "Epoch 222/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9952\n",
      "Epoch 223/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9935 - val_loss: 0.0166 - val_fmeasure: 0.9951\n",
      "Epoch 224/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9936 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 225/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9939 - val_loss: 0.0169 - val_fmeasure: 0.9951\n",
      "Epoch 226/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9937 - val_loss: 0.0163 - val_fmeasure: 0.9953\n",
      "Epoch 227/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0169 - val_fmeasure: 0.9951\n",
      "Epoch 228/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9940 - val_loss: 0.0165 - val_fmeasure: 0.9955\n",
      "Epoch 229/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9938 - val_loss: 0.0163 - val_fmeasure: 0.9954\n",
      "Epoch 230/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9937 - val_loss: 0.0166 - val_fmeasure: 0.9953\n",
      "Epoch 231/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9938 - val_loss: 0.0163 - val_fmeasure: 0.9955\n",
      "Epoch 232/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0168 - val_fmeasure: 0.9950\n",
      "Epoch 233/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9937 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 234/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9937 - val_loss: 0.0167 - val_fmeasure: 0.9956\n",
      "Epoch 235/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9954\n",
      "Epoch 236/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9937 - val_loss: 0.0164 - val_fmeasure: 0.9954\n",
      "Epoch 237/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9952\n",
      "Epoch 238/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9955\n",
      "Epoch 239/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9938 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 240/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9955\n",
      "Epoch 241/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9939 - val_loss: 0.0166 - val_fmeasure: 0.9957\n",
      "Epoch 242/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9937 - val_loss: 0.0163 - val_fmeasure: 0.9955\n",
      "Epoch 243/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9939 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 244/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9937 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 245/331\n",
      "2s - loss: 0.0367 - fmeasure: 0.9936 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 246/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9938 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 247/331\n",
      "2s - loss: 0.0366 - fmeasure: 0.9936 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 248/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9939 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 249/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9937 - val_loss: 0.0164 - val_fmeasure: 0.9957\n",
      "Epoch 250/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 251/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9939 - val_loss: 0.0163 - val_fmeasure: 0.9955\n",
      "Epoch 252/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9938 - val_loss: 0.0168 - val_fmeasure: 0.9952\n",
      "Epoch 253/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9937 - val_loss: 0.0167 - val_fmeasure: 0.9954\n",
      "Epoch 254/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0164 - val_fmeasure: 0.9953\n",
      "Epoch 255/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9940 - val_loss: 0.0165 - val_fmeasure: 0.9952\n",
      "Epoch 256/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 257/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9953\n",
      "Epoch 258/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 259/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0163 - val_fmeasure: 0.9955\n",
      "Epoch 260/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9939 - val_loss: 0.0161 - val_fmeasure: 0.9954\n",
      "Epoch 261/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9938 - val_loss: 0.0166 - val_fmeasure: 0.9952\n",
      "Epoch 262/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9936 - val_loss: 0.0165 - val_fmeasure: 0.9956\n",
      "Epoch 263/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9937 - val_loss: 0.0164 - val_fmeasure: 0.9954\n",
      "Epoch 264/331\n",
      "3s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9957\n",
      "Epoch 265/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 266/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0162 - val_fmeasure: 0.9956\n",
      "Epoch 267/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9939 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 268/331\n",
      "3s - loss: 0.0362 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 269/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9938 - val_loss: 0.0166 - val_fmeasure: 0.9953\n",
      "Epoch 270/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9937 - val_loss: 0.0166 - val_fmeasure: 0.9952\n",
      "Epoch 271/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9939 - val_loss: 0.0165 - val_fmeasure: 0.9956\n",
      "Epoch 272/331\n",
      "2s - loss: 0.0364 - fmeasure: 0.9938 - val_loss: 0.0166 - val_fmeasure: 0.9953\n",
      "Epoch 273/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9937 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 274/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 275/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9939 - val_loss: 0.0164 - val_fmeasure: 0.9954\n",
      "Epoch 276/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9939 - val_loss: 0.0165 - val_fmeasure: 0.9954\n",
      "Epoch 277/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9936 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 278/331\n",
      "2s - loss: 0.0358 - fmeasure: 0.9939 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 279/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 280/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9959\n",
      "Epoch 281/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9939 - val_loss: 0.0164 - val_fmeasure: 0.9952\n",
      "Epoch 282/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9954\n",
      "Epoch 283/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9956\n",
      "Epoch 284/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9937 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 285/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9938 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 286/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9936 - val_loss: 0.0164 - val_fmeasure: 0.9954\n",
      "Epoch 287/331\n",
      "2s - loss: 0.0365 - fmeasure: 0.9936 - val_loss: 0.0162 - val_fmeasure: 0.9956\n",
      "Epoch 288/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 289/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0162 - val_fmeasure: 0.9954\n",
      "Epoch 290/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9937 - val_loss: 0.0163 - val_fmeasure: 0.9952\n",
      "Epoch 291/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0162 - val_fmeasure: 0.9953\n",
      "Epoch 292/331\n",
      "2s - loss: 0.0363 - fmeasure: 0.9937 - val_loss: 0.0161 - val_fmeasure: 0.9956\n",
      "Epoch 293/331\n",
      "2s - loss: 0.0358 - fmeasure: 0.9939 - val_loss: 0.0162 - val_fmeasure: 0.9956\n",
      "Epoch 294/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9939 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 295/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9938 - val_loss: 0.0163 - val_fmeasure: 0.9957\n",
      "Epoch 296/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9937 - val_loss: 0.0168 - val_fmeasure: 0.9955\n",
      "Epoch 297/331\n",
      "2s - loss: 0.0357 - fmeasure: 0.9939 - val_loss: 0.0168 - val_fmeasure: 0.9954\n",
      "Epoch 298/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9937 - val_loss: 0.0165 - val_fmeasure: 0.9955\n",
      "Epoch 299/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9958\n",
      "Epoch 300/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9939 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 301/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9939 - val_loss: 0.0163 - val_fmeasure: 0.9953\n",
      "Epoch 302/331\n",
      "3s - loss: 0.0361 - fmeasure: 0.9938 - val_loss: 0.0163 - val_fmeasure: 0.9953\n",
      "Epoch 303/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0163 - val_fmeasure: 0.9958\n",
      "Epoch 304/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9939 - val_loss: 0.0164 - val_fmeasure: 0.9953\n",
      "Epoch 305/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 306/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9937 - val_loss: 0.0166 - val_fmeasure: 0.9955\n",
      "Epoch 307/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 308/331\n",
      "3s - loss: 0.0360 - fmeasure: 0.9938 - val_loss: 0.0158 - val_fmeasure: 0.9957\n",
      "Epoch 309/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9937 - val_loss: 0.0160 - val_fmeasure: 0.9956\n",
      "Epoch 310/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9939 - val_loss: 0.0163 - val_fmeasure: 0.9954\n",
      "Epoch 311/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9937 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 312/331\n",
      "2s - loss: 0.0361 - fmeasure: 0.9939 - val_loss: 0.0165 - val_fmeasure: 0.9954\n",
      "Epoch 313/331\n",
      "2s - loss: 0.0362 - fmeasure: 0.9936 - val_loss: 0.0161 - val_fmeasure: 0.9956\n",
      "Epoch 314/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 315/331\n",
      "2s - loss: 0.0357 - fmeasure: 0.9939 - val_loss: 0.0162 - val_fmeasure: 0.9954\n",
      "Epoch 316/331\n",
      "3s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0162 - val_fmeasure: 0.9954\n",
      "Epoch 317/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9938 - val_loss: 0.0164 - val_fmeasure: 0.9953\n",
      "Epoch 318/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9939 - val_loss: 0.0162 - val_fmeasure: 0.9956\n",
      "Epoch 319/331\n",
      "2s - loss: 0.0358 - fmeasure: 0.9939 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 320/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 321/331\n",
      "2s - loss: 0.0358 - fmeasure: 0.9937 - val_loss: 0.0163 - val_fmeasure: 0.9954\n",
      "Epoch 322/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9939 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 323/331\n",
      "2s - loss: 0.0358 - fmeasure: 0.9938 - val_loss: 0.0160 - val_fmeasure: 0.9956\n",
      "Epoch 324/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9937 - val_loss: 0.0162 - val_fmeasure: 0.9955\n",
      "Epoch 325/331\n",
      "2s - loss: 0.0357 - fmeasure: 0.9940 - val_loss: 0.0165 - val_fmeasure: 0.9952\n",
      "Epoch 326/331\n",
      "2s - loss: 0.0357 - fmeasure: 0.9939 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 327/331\n",
      "3s - loss: 0.0360 - fmeasure: 0.9939 - val_loss: 0.0162 - val_fmeasure: 0.9956\n",
      "Epoch 328/331\n",
      "2s - loss: 0.0358 - fmeasure: 0.9940 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 329/331\n",
      "2s - loss: 0.0360 - fmeasure: 0.9938 - val_loss: 0.0163 - val_fmeasure: 0.9954\n",
      "Epoch 330/331\n",
      "2s - loss: 0.0359 - fmeasure: 0.9938 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 331/331\n",
      "2s - loss: 0.0358 - fmeasure: 0.9939 - val_loss: 0.0166 - val_fmeasure: 0.9954\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, nb_epoch = 331, batch_size = 1000, verbose = 2, \n",
    "                    validation_split = .20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Neural Network achieves an f1 score of  0.995686564884 on the oversampled testing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[84716,   698],\n",
       "       [   36, 85139]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions[:] = predictions[:]>0.5\n",
    "\n",
    "print \"Our Neural Network achieves an f1 score of \" , f1_score(y_test, predictions, pos_label = 0) , \"on the oversampled testing data\"\n",
    "confusion_matrix(y_test, predictions)\n",
    "\n",
    "# add f1 score on original data set, train test split on features labels earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VdXV+PHvCglhyBymEEiYB0HBCagViVAVB8Q6VFBB\nrW19HarVtoq+RVBrLf1Vq9a2OOCAE84WhyoWiegLyqAgU5hJwpQBSEgCmdfvj30SLzGBEHJzc8P6\nPM99uPecffZZ59xw1t17n0FUFWOMMeZohQQ6AGOMMcHJEogxxpgGsQRijDGmQSyBGGOMaRBLIMYY\nYxrEEogxxpgGsQRiGkxEtorI6DrmnSki65o6JuM/IvK8iDwQ6DhM82EJxPiFqn6pqgMDHUcwE5FR\nIpIZ6DiMqYslEBN0RET8VG8rf9R7DAQ47JW+zTBmcxyxBGKO1TARWSMie0Rkloi0hh/+eva6u34r\nIitFZJ+IvOZTNkZE3heRbK+e90Uk0WfZBSLyRxH5UkSKgN+KyDLfIETkThF5t7YARSRWRJ4TkR1e\n/e/4xigid4nILuA5b/ovRWSjiOSKyHsikuBT199EJEtE8r1tOcGbfoG3H/Z7dd7ps8xFIvKtt91f\nisiJh9kvc0SktYi0Az4CuopIgVdvFxGZJiJvishLIpIHXOuVf8zbvu1ejGE1tvEeEckRkS0icpU3\n7zQR2e2bkEXkUhFZUZ8vvrH3kwlCqmovezXoBWwFvgO6AjHAl8AD3rxRQEaNsl8Bnb2ya4FfefPi\ngJ8C4UB74HXgXZ9lFwDbgAG4Hz2tgVygv0+Zb4BL6ojzQ+A1IApoBYz0ibEM+BMQ5q1/NJADDPGm\nPQF87pU/F1gKRHqf+wOdvfc7gTO899HAUO/9yUAWcBquRTHJ2xdh9dgvh+xDb9o0oAQY531uAzwA\nLALivdf/AffX2Mb/523PWUAh0Nebvxo4z6f+d4Df1LEfn/f5fht1P9krOF/WAjHH6u+qulNV84CH\ngImHKfu4qmZ5Zd8HhgKo6l5VfVdVS1S1CHgYd6Dz9YKqpqlqpaqW4pLMNQAiMghIxiWKQ4hIF+A8\n4EZV3a+qFar6hU+RCmCaqpapaglwFTBLVVeqahlwDzBCRJJwB+JI4AQREVVdr6pZXj2lwCARiVTV\nfFWt+hX/S2Cmqi5T5yVcAhhxpP1yGItV9X1v3xV7Md+vqntUdQ9wPy5RVVFgqreNC7399DNv3uyq\nsiIS5+2r146wfvywn0wQsgRijtV2n/fpuNZIXbJ83h8AIgBEpK2IPCUi27xumc+BmBpjHTUHk2fj\nDmLgEskb3oGspu7AXlXdX0dMOTWW6+ptBwBeQtsLJKrqAuBJ4B9AlojMFJEIr+hlwIVAutflVpUg\nknFdbnu91z6gG4fup1r3y2HU3BddgQyfzzW/h31eoqlt/svARSLSFpdUFvoc7A+nsfeTCUKWQMyx\n6u7zPhnXRXG0fgf0BU5X1Ri+b334JpBDBpNV9WugVERG4hLJS3XUnQnEiUhUHfNrDlLvxG2HC0Ck\nPa5baIe33idV9TTgBFzXzO+96ctV9RKgI/Bv4A2f9T+kqnHeK1ZVI1T19TriOVxsdU3f4RszP/we\nYr0EUSWpar6q7gQW4w7s11D3fqypsfeTCUKWQMyxukVEEr3uj3uBOQ2oIwI4COz36plez+Vewv3S\nLVXVRbUVUNXdwH+Af3qD9aFe0qnLa8D1InKSiITjxkcWq2qGN+g8TERCvXiLgUoRCRORq0QkSlUr\ngAJc1xjAM8D/iMgwcAdabyC5fT22LwuIP0zyqzIH+IOIdBCRDsBUDk0EAtzvxTkS1wJ402f+S8Bd\nwGDcGEh9NPZ+MkHIEog5Fgq8CswDNgEbceMgdZWty2NAO9zA+CLc2Uf1WfYl3EHvSL+aJwHlQBru\noHx7XQVVdT7uAPwO7td0T74f14nCJYS9uMHvXNzgdNU6tnpdcL/C615T1eW4cZAnRWQvsAG4th7b\nhqquxx2ot3jdX13qKPpHYBnuhIaV3nvf72EXsA/XangJNx60wWf+u7jWxDs1urp+EJJPbI26n0xw\nElX/PlBKRMbiDhAhuEG3GTXm98ed3XEKcK+qPuozLxp4FneQqAR+7nVdGIOItMElhFNUdXOg42mO\nRGQU8JKqJh2h3Cbc2V+fNU1kpiUI9WflIhKC62IYg/v1s1RE/q2qaT7F9gC/Bi6ppYrHgY9U9Qqv\nOdzOn/GaoHMzsNSSx7ERkcuASkse5mj5NYEAw4CNqpoOICJzgPG4rgQAVDUXyBWRi3wX9Pp9R6rq\ndV65cqCuM2nMcUZEtnpva/vhYepJRBYAA/FOiTbmaPg7gSRy6CmH23FJpT564hLL87iLlZYBt6vq\nwcYN0QQjVe0Z6BiCgap+jjvrqq75ZzdhOKaFac6D6KG4cZF/qOopuPPjpwQ2JGOMMVX83QLZwaG/\nfrp50+pjO5CpqlX3PHoLuLu2giLi3zMBjDGmBVLVY7oxqb9bIEuBPiKSLO7GeROAuYcpX70x3tWw\nmSLSz5s0BnefoFo1xX1f/PGaNm1awGOw+AMfh8UfnK9gjr8x+LUFoqoVInIr7jqBqtN414nIjW62\nPi0inXHjG5G4i41uB05Q1ULgNuAV786iW4Dr/RmvMcaY+vN3Fxaq+jHuVga+057yeZ/FobfD8C23\nEjjdrwEaY4xpkOY8iH5cSElJCXQIx8TiDyyLP7CCPf5j5fcr0ZuCu2N08G+HMcY0FRFBj3EQ3e9d\nWMaYwOjRowfp6elHLmhatOTkZLZt2+aXuq0FYkwL5f3CDHQYJsDq+jtojBaIjYEYY4xpEEsgxhhj\nGsQSiDHGmAaxBGKMCVo33XQTDz1U1zPMGl72aKSnpxMSEkJlZWWj193c2SC6MS1Ucx9E79mzJ7Nm\nzWL06NGBDuWYpKen06tXL8rKyggJaX6/yW0Q3Rhz3KmosMelN3eWQIwxTW7y5MlkZGQwbtw4oqKi\n+Otf/1rdFfTcc8+RnJzMmDFjAPjZz35GQkICsbGxpKSksHbt9/dUvf7667nvvvsA+Pzzz+nevTuP\nPvoonTt3JjExkRdeeKFBZffu3cu4ceOIjo5m+PDhTJ06lZEjR9Zr23bt2sX48eOJj4+nX79+PPvs\ns9Xzli5dyumnn050dDQJCQn87ne/A6CkpIRJkybRoUMHYmNjGT58ODk5OQ3at03JEogxpsnNnj2b\npKQkPvjgA/bv3199IAVYuHAhaWlpfPLJJwBccMEFbN68mezsbE455RSuvvrqOuvdvXs3BQUF7Ny5\nk2effZZbbrmF/Pz8oy578803ExkZSXZ2Ni+88AIvvvgiIvXr7bnyyitJSkpi9+7dvPnmm9x7772k\npqYCcPvtt/Ob3/yG/Px8Nm/ezM9+9jMAXnzxRfbv38+OHTvYu3cvM2fOpG3btvVaXyBZAjHmOCbS\nOK+Gqtk3LyLcf//9tG3blvDwcACuu+462rVrR1hYGPfddx8rV66koKCg1vpat27N1KlTadWqFeef\nfz4RERGsX7/+qMpWVlbyzjvv8MADDxAeHs7AgQO59tpr67U9mZmZLF68mBkzZhAWFsaQIUP4xS9+\nwezZswEICwtj06ZN7Nmzh3bt2jFs2LDq6Xv27GHDhg2ICCeffDIRERH1WmcgWQIx5jim2jivxtSt\nW7fq95WVlUyZMoU+ffoQExNDz549ERFyc3NrXTY+Pv6Qgex27dpRWFh4VGVzcnKoqKg4JI7u3Wu9\nYfgP7Nq1i7i4ONq1a1c9LTk5mR073HP0nnvuOdavX8+AAQMYPnw4H374IQCTJk3ivPPOY8KECXTr\n1o0pU6YExRiQJRBjTEDU1SXkO/3VV1/l/fff57PPPiMvL49t27Y16gORatOxY0dCQ0PZvn179bTM\nzMx6Ldu1a1f27t1LUVFR9bSMjAwSExMB6N27N6+++io5OTncddddXH755Rw8eJDQ0FCmTp3KmjVr\nWLRoEe+//351q6U5swRijAmILl26sGXLlkOm1UwMBQUFhIeHExsbS1FREffcc0+9xyIaKiQkhEsv\nvZTp06dz8OBB0tLSjngwr4q7W7dunHHGGdxzzz2UlJTw3XffMWvWLCZNmgTAK6+8Ut16io6ORkQI\nCQkhNTWV1atXU1lZSUREBGFhYc3ylOCamn+ExpgWacqUKTz44IPExcXx6KOPAj9slUyePJmkpCQS\nExMZPHgwZ5xxxlGt42iSjW/Zv//97+Tl5ZGQkMC1117LVVddVT0mc6RlX3vtNbZu3UrXrl257LLL\nePDBBzn77LMB+Pjjjxk0aBBRUVHccccdvP7664SHh7N7924uv/xyoqOjGTRoEGeffXZ10mnO7EJC\nY1qo5n4hYTCZMmUKWVlZPP/884EO5ajZhYTGGNOE1q9fz6pVqwBYsmQJs2bN4tJLLw1wVM2P3xOI\niIwVkTQR2SAid9cyv7+ILBKRYhG5s5b5ISLyjYjM9XesxhgDbuzl0ksvJSIigokTJ/L73/+ecePG\nBTqsZsevXVgiEgJsAMYAO4GlwARVTfMp0wFIBi4B9qnqozXquAM4FYhS1YvrWI91YRlTg3VhGQju\nLqxhwEZVTVfVMmAOMN63gKrmqupyoLzmwiLSDbgAeLbmPGOMMYHl7wSSCPieQL3dm1ZffwN+D9jP\nKGOMaWaa7SC6iFwIZKnqCkC8lzHGmGYi1M/17wCSfD5386bVx4+Bi0XkAqAtECkis1V1cm2Fp0+f\nXv0+JSWFlJSUhsRrjDEtUmpqavVNHRuLvwfRWwHrcYPou4AlwERVXVdL2WlAoao+Usu8UcBvbRDd\nmPqzQXQDQTyIrqoVwK3APGANMEdV14nIjSLyKwAR6SwimcAdwP+KSIaINP/bUBpjAqLqWR5VBg8e\nzMKFC+tV9mj56zG4999/f1BcaX4k/u7CQlU/BvrXmPaUz/ss4LDfsKp+DnzulwCNMUHH99Yhq1ev\nrnfZw3nxxRd59tln+eKLL6qn/etf/2pYgPXg73t6NYVmO4hujDFNSVVbxEG9KVkCMcY0ub/85S9c\nccUVh0yrelofwAsvvMAJJ5xAVFQUffr04emnn66zrp49e/LZZ58BUFxczHXXXUdcXByDBw9m6dKl\nh5SdMWMGffr0ISoqisGDB/Pee+8BkJaWxk033cTixYuJjIwkLi4OOPQxuADPPPMMffv2pUOHDlxy\nySXs2rWrel5ISAhPPfUU/fr1Iy4ujltvvbXe+2Pu3LkMHjyYuLg4Ro8eTVpa9bXWzJgxg27duhEV\nFcXAgQNZsGABUPfjcZtU1b31g/nlNsMY46s5/79IT0/X9u3ba2FhoaqqVlRUaEJCgi5ZskRVVT/6\n6CPdunWrqqouXLhQ27Vrp99++62qqqampmr37t2r6+rRo4fOnz9fVVXvvvtuPeusszQvL0+3b9+u\ngwcPPqTsW2+9pbt371ZV1TfeeEPbt29f/fmFF17QkSNHHhLnddddp1OnTlVV1fnz52uHDh10xYoV\nWlpaqr/+9a/1rLPOqi4rIjpu3Djdv3+/ZmRkaMeOHfWTTz6pdfunT5+ukyZNUlXV9evXa/v27XX+\n/PlaXl6uf/nLX7RPnz5aVlam69ev1+7du1fHmJ6erlu2bFFV1R/96Ef68ssvq6pqUVGRfv3117Wu\nq66/A2/6MR17/T4GYoxpvuT+xumy0WlHd7ZXUlISp5xyCu+++y7XXHMN8+fPp3379px++ukAnH/+\n+dVlR44cybnnnssXX3zB0KFDD1vvm2++ycyZM4mOjiY6OprbbruNBx98sHr+ZZddVv3+iiuu4E9/\n+hNLliyp132uXn31VW644QaGDBkCwMMPP0xsbCwZGRkkJbmrFe655x4iIyOJjIzk7LPPZsWKFZx7\n7rmHrfeNN97goosuYvTo0QD87ne/4/HHH2fRokUkJiZSWlrK6tWriY+Pr14PuEfyVj0eNz4+vvrx\nuE3JEogxx7GjPfA3pokTJ/Laa69xzTXX8Nprr3HVVVdVz/vPf/7DAw88wIYNG6isrOTgwYOcdNJJ\nR6xz586dhzyKNjk5+ZD5s2fP5m9/+xvbtm0DoKioqM7H49ZW96mnnlr9uX379sTHx7Njx47qA3vn\nzp2r5x/ucbo16/WNU0To3r07O3bs4KyzzuKxxx5j+vTprF27lvPOO49HHnmEhIQEZs2axdSpUxkw\nYAC9evXivvvu48ILL6zXtjQWGwMxxgTEFVdcQWpqKjt27ODdd9+tTiClpaVcfvnl3HXXXeTk5LBv\n3z7OP//8el3TkpCQcMjjZ9PT06vfZ2Rk8Ktf/Yp//vOf7Nu3j3379jFo0KDqeo80gN61a9dD6isq\nKmLPnj2HJKyGqFkvuEfoVj0Gd8KECXzxxRfVZaZMmQLU/XjcpmQJxBgTEB06dGDUqFFcf/319OrV\ni/793dn+paWllJaW0qFDB0JCQvjPf/7DvHnz6lXnz372Mx5++GHy8vLYvn07Tz75ZPW8oqIiQkJC\n6NChA5WVlTz//POHnALcuXNntm/fTllZWa11T5w4keeff57vvvuOkpIS7r33XkaMGHFM15lUxfzh\nhx+yYMECysvL+etf/0qbNm0444wz2LBhAwsWLKC0tJTWrVvTtm3b6kfd1vV43KZkCcQYEzBXXXUV\n8+fP5+qrr66eFhERwRNPPMEVV1xBXFwcc+bMYfz48XXW4dtymDZtGklJSfTs2ZOxY8cyefL3dz4a\nOHAgv/3tbxkxYgRdunRhzZo1nHnmmdXzR48ezaBBg+jSpQudOnX6wXrGjBnDgw8+yKWXXkpiYiJb\nt25lzpw5tcZR2+e69OvXj5dffplbb72Vjh078uGHH/L+++8TGhpKSUkJU6ZMoWPHjnTt2pWcnBwe\nfvhhoO7H4zYle6StMS2U3crEQBDfysQYY0zLZQnEGGNMg1gCMcYY0yCWQIwxxjSIJRBjjDENYgnE\nGGNMg9itTIxpoZKTk+325OYHt3NpTHYdiDHGHIfsOhBjjDEBYwnEGGNMg/g9gYjIWBFJE5ENInJ3\nLfP7i8giESkWkTt9pncTkc9EZI2IrBKR2/wdqzHGmPrz6xiIiIQAG4AxwE5gKTBBVdN8ynQAkoFL\ngH2q+qg3vQvQRVVXiEgEsBwY77usTx02BmKMMUchGMZAhgEbVTVdVcuAOcAht9VU1VxVXQ6U15i+\nW1VXeO8LgXVAop/jNcYYU0/+TiCJQKbP5+00IAmISA9gKPB1o0RljDHmmDX760C87qu3gNu9lkit\npk+fXv0+JSWFlJQUv8dmjDHBIjU1ldTU1Eat099jICOA6ao61vs8BVBVnVFL2WlAQdUYiDctFPgA\n+I+qPn6Y9dgYiDHGHIVgGANZCvQRkWQRaQ1MAOYepnzNjXkOWHu45GGMMSYw/H4luoiMBR7HJatZ\nqvpnEbkR1xJ5WkQ6A8uASKASKAROAIYAC4FVgHqve1X141rWYS0QY4w5Co3RArFbmRhjzHEoGLqw\njDHGtFCWQIwxxjSIJRBjjDENYgnEGGNMg1gCMcYY0yCWQIwxxjSIJRBjjDENYgnEGGNMg1gCMcYY\n0yCWQIwxxjSIJRBjjDENYgnEGGNMg1gCMcYY0yCWQIwxxjSIJRBjjDENYgnEGGNMg1gCMcYY0yCW\nQIwxxjSIJRBjjDEN4vcEIiJjRSRNRDaIyN21zO8vIotEpFhE7jyaZY0xxgSOqKr/KhcJATYAY4Cd\nwFJggqqm+ZTpACQDlwD7VPXR+i7rU4f6czuMMaalERFUVY6lDn+3QIYBG1U1XVXLgDnAeN8Cqpqr\nqsuB8qNd1hhjTOD4O4EkApk+n7d70/y9rDHGGD8LDXQAjWX69OnV71NSUkhJSQlYLMYY09ykpqaS\nmpraqHX6ewxkBDBdVcd6n6cAqqozaik7DSjwGQM5mmVtDMQYY45CMIyBLAX6iEiyiLQGJgBzD1Pe\nd2OOdlljjDFNyK9dWKpaISK3AvNwyWqWqq4TkRvdbH1aRDoDy4BIoFJEbgdOUNXC2pb1Z7zGGGPq\nz69dWE1FRLSyUpFjaowZY8zxIxi6sJpMZWWgIzDGmONLi0kgFRWBjsAYY44vLSaBlJYGOgJjjDm+\ntJgEUlQU6AiMMeb40mISSGFhoCMwxpjjS4tJINYCMcaYpmUJxBhjTIO0mARiXVjGGNO0WkwCsRaI\nMcY0rRaTQKwFYowxTavFJBBrgRhjTNNqMQnEWiDGGNO0WkwCsRaIMcY0rRaTQKwFYowxTavFJBBr\ngRhjTNNqMQnEWiDGGNO06pVAROR2EYkSZ5aIfCMi5/o7uKNhLRBjjGla9W2B/FxV9wPnArHAJODP\nfouqAawFYowxTau+CaTqsYcXAC+p6hqfac2CtUCMMaZp1TeBLBeRebgE8omIRAL1eoisiIwVkTQR\n2SAid9dR5gkR2SgiK0RkqM/0O0RktYh8JyKviEjrutZjCcQYY5pWfRPIDcAU4HRVPQCEAdcfaSER\nCQGeBM4DBgETRWRAjTLnA71VtS9wIzDTm94V+DVwiqqeBIQCE+pal3VhGWNM06pvAvkRsF5V80Tk\nGuAPQH49lhsGbFTVdFUtA+YA42uUGQ/MBlDVr4FoEenszWsFtBeRUKAdsLOuFVkLxBhjmlZ9E8i/\ngAMiMgT4LbAZ76B/BIlAps/n7d60w5XZASSq6k7gESDDm5anqv+ta0XWAjHGmKYVWs9y5aqqIjIe\neFJVZ4nIDf4MTERicK2TZFxr5y0RuUpVX62tfEHBdKZNAxFISUkhJSXFn+EZY0xQSU1NJTU1tVHr\nFFU9ciGRz4GPgZ8DI4FsYKWqnniE5UYA01V1rPd5CqCqOsOnzExggaq+7n1OA0Z56zlPVX/pTZ8E\nDFfVW2tZj8bEKJs2QXx8PbbaGGOOcyKCqh7T2bT17cK6EijBXQ+yG+gG/L96LLcU6CMiyd4ZVBOA\nuTXKzAUmQ3XCyVPVLFzX1QgRaSMiAowB1tW1os6dISurnltjjDHmmNUrgXhJ4xXcAPdFQLGqHnEM\nRFUrgFuBecAaYI6qrhORG0XkV16Zj4CtIrIJeAq42Zu+BHgL+BZYibvu5Om61mUJxBhjmlZ9u7B+\nhmtxpOIO5COB36vqW36Nrp5ERK+4Qrn0UphQ54m+xhhjqjRGF1Z9B9H/F3cNSLa34o7Af3EthGbB\nWiDGGNO06jsGElKVPDx7jmLZJmEJxBhjmlZ9WyAfi8gnwGve5yuBj/wTUsN07gxffRXoKIwx5vhR\nrwSiqr8XkcuAH3uTnlbVd/0X1tHr3Bmys49czhhjTOOobwsEVX0beNuPsRwT68IyxpimddgEIiIF\nQG2naQnugsAov0TVAJZAjDGmadXrNN7mTkT04EElJgYOHICQZjW8b4wxzU9TXone7LVpA9HRNg5i\njDFNpcUkEICkJEhPD3QUxhhzfGhxCSQjI9BRGGPM8cESiDHGmAZpUQkkOdm6sIwxpqm0qARiLRBj\njGk6lkCMMcY0SItKID16wJYt0AIubTHGmGavRSWQDh0gNNSuSDfGmKbQohIIwIABsK7OB98aY4xp\nLC0ugQwcaAnEGGOagiUQY4wxDeL3BCIiY0UkTUQ2iMjddZR5QkQ2isgKERnqMz1aRN4UkXUiskZE\nhh9pfZZAjDGmafg1gYhICPAkcB4wCJgoIgNqlDkf6K2qfYEbgZk+sx8HPlLVgcAQ4IipYcgQ+PZb\nqKhopI0wxhhTK3+3QIYBG1U1XVXLgDnA+BplxgOzAVT1ayBaRDqLSBQwUlWf9+aVq+r+I62wa1dI\nTIQlSxp1O4wxxtTg7wSSCGT6fN7uTTtcmR3etJ5Arog8LyLfiMjTItK2Piu94AL4qFk9sd0YY1qe\nej/SNgBCgVOAW1R1mYg8BkwBptVWePr06dXvu3ZNYdasFB58sCnCNMaY5i81NZXU1NRGrdOvTyQU\nkRHAdFUd632egnsU7gyfMjOBBar6uvc5DRjlzV6sqr286WcCd6vquFrWo77bUVnprkr/4AM46ST/\nbJsxxgSzYHgi4VKgj4gki0hrYAIwt0aZucBkqE44eaqapapZQKaI9PPKjQHW1melISEweTK8+GKj\nbIMxxpha+P2Z6CIyFnc2VQgwS1X/LCI34loiT3tlngTGAkXA9ar6jTd9CPAsEAZs8ebl17IOrbkd\nq1fDuHGwdav/ts0YY4JVY7RA/J5AmkJtCUQVOneGpUvdc0KMMcZ8Lxi6sAJGBFJS4PPPAx2JMca0\nTC02gYBLIJ99FugojDGmZWqxXVgAO3bAiSdCWhp06hSAwIwxppmyLqwjSEyEq6+Gv/wl0JEYY0zL\n06JbIAA7d8LgwbBmDSQkNHFgxhjTTNlZWB4R0crKSkRq3xd33AF5efDcc25w3RhjjnfWheWjoLSg\nznnTp8PKlfDQQ00XjzHGtHQtJoFkFdb9IPToaHdzxX/+Ez7+uAmDMsaYFqzFJJDdhbsPO79LF3j9\ndXeLk/fea6KgjDGmBTtuEgjAyJEwdy7cdJMbEzHGGNNwx1UCARgxAi6/3D36ds4cPwdljDEtWItJ\nIFlFdY+B1PT3v8O//w2//jUsXOjHoIwxpgVrMQmkvi2QKsOGwauvutbI6NHwyivuBozGGGPqpzk/\nkfCoHG0CATjnHHd677JlcNddsHkz3HefH4IzxpgW6LhtgVRJSHDPDUlNda2Qfv3g7LPdlevGGGPq\ndtwnkCqdO8PatfDOO65b65xz4JFHYNeuRgrQGGNamBbThZVdlE2lVhIiDc+JrVq5+2YNHgxDh8Ks\nWTBoEFxxBfTsCTfeCLGxjRi0McYEsRbTAoltG3vYq9GP1o9/7O6dtXYt9OgB69e7f1NS4M47Xcuk\nuLjRVmeMMUGnxbRAkqKTyNyfSUJk495yt0sXuOce9/6RR2D5cnj7bejfH9q3dxcndu8O114LRUXw\nox816uqNMabZ8nsLRETGikiaiGwQkbvrKPOEiGwUkRUiMrTGvBAR+UZE5h5uPd2jupORn9GYof9A\nXJwbG5k5E/bvd4mk6rG5KSkwYYJLKH//O6xbB99++30rxU4RNsa0NH5tgYhICPAkMAbYCSwVkX+r\nappPmfNtrQZRAAAdQklEQVSB3qraV0SGAzOBET7V3A6sBaIOt67uUd3JzM9s7E04rDPOcK+JE+Hg\nQYiPh08/hTffhMceg8pK9yTENm3g66/h9NNh1Cj473/deMq110JIi+lENMYcb/zdhTUM2Kiq6QAi\nMgcYD6T5lBkPzAZQ1a9FJFpEOqtqloh0Ay4AHgLuPNyKukd3J3N/0yaQKrGx3w+uX3SRe4FLIE89\n5brBzj4bvvjCJY877oBHH4UHHnAtmVGj3CB9165urOXUU119/fu7Fkzv3tChQ0A2zRhj6uTvBJII\n+B7Vt+OSyuHK7PCmZQF/A34PRB9pRd2juvP1jq+PKdjGFhLibtxYZdw49wJ3ZteyZa6F8tVXsGED\nrFrlzgBbsgRycmDbNggPhy1boG9f6NgRVqyA4cNduVWr4IILXNm2beG886BXL5d0og7bXjPGmGPX\nbAfRReRCIEtVV4hICnDYJ2fNmzWPRZsXMX3NdFJSUkhJSWmKMBssJMTdTgXc2V2Hs2MHpKfD9u0w\ndap7tklpqWvpvPqqa6mUlblB/p07XZfZxInuyvp+/b7vSuvf3yWiDz5wdZ58MkRGwkknQbdubtn+\n/aFdO79vvjGmiaWmppKamtqodfr1kbYiMgKYrqpjvc9TAFXVGT5lZgILVPV173MaMAo39nENUA60\nBSKBd1R1ci3r0cz8TIbMHMLyXy2nR0wPv21Tc6fqWi7PPutaKpmZ7vqW7Gw3qJ+R4Qb6TzjBfS4q\ngs8+gwMHXBfa7t0usaWnu6SSmQknngiLF7vEVFYGV14JMTEwYIBrHRUVuRZPt26uxRQf75JWbKxr\nEbVp42LbssV1xVnryJjAa/bPRBeRVsB63CD6LmAJMFFV1/mUuQC4RVUv9BLOY6o6okY9o4DfqurF\ndaxHKysrmZ46nVnfzmL7ndv9tUktUmWle1a8iEsYy5ZBUpJ7n5DgTl0eNcrNP3DAPZCroMB1u/Xv\n71oxOTku+Rw4APn5Lmnt2eNaOqru9OZvvoGKCtdyWr/eJbiPP3atqX79XGJr3dpde7NqlUtWISHu\n38mT3cWdAwbAxo2wb5+L58wz3TbExLiTF0aPdrF17OiSZXY2DBkCoaFQXu5ijT5ih6gxLV+zTyDg\nTuMFHsedMjxLVf8sIjfiWiJPe2WeBMYCRcD1qvpNjTqOmEBUFVUlZkYMW27bQny7eH9ulqmnwkI3\nJvP22+7gHhMDL73kEsaiRfDTn7qD/Zo17nNZmXtWy4knusRTVOTqWLgQ/vMfl5QGD3atm4oKd9Zb\nWJhLWsOHu2TXtat7YFhSkmsNrV7tYmnVyp20cOKJrrW0ZIlrMbVrB1lZruXVpcv3ibNVKxdv586w\naZOrr6TEzdu1yyW0775zrbWEBBfvhg0unptuci27JUsgMdF1F44c6VphW7e6JNqqlUuEmzbBhRe6\n9c6f7xLjpElu2ysrXUzgEuPevS72bdvcNm/Z4uo577zaux5V3f5v2/b7afv2uWWP1HVqWragSCBN\noSqBAAx7ZhiPjX2MM7qfEeCojD+ougNslYoK10opKHCti4oK19qoS0mJS0ZVXXX797tpycnugF9U\n5C4MzfRO68jPd8miZ0/XmgoPd2NRHTu65DF0qJuXne3iqDq4f/yxS1QXX+wSxsqVMG+eK9uvn2st\nRUa6RBgX57oTIyPh/PNdkpg7182rrHStsogIyM39/t/oaJfYEhNdC66qKzEszH0+8US3TzZudK3C\nqCjo08fVtWKFKzdsmNv2pCTXgszOdq+OHeG221wc33wDP/mJu9apWzc47TQ3hibiYoiJcXUsX+62\nPyzMJfARI75Pzk895fZjTIz7EXHmmW5f9O/vti831103dd557sdCRYVLikuXutbjwYOu3hEj3HZU\nVLhtyslxPxQyMlwCrzqZ5OSTXd35+a77tF07V29Z2fdnS1ZUuFbwwYOQlub21XXXue3KynLfWe/e\nbpvBJd3o6O//DQ11f4uFhe472bvXfY+Fhe57LCx0sXXt6r6XfftcHM3pbEpLIB7fBDLp3UmM7jGa\n60++PsBRGVO3qoNklfR0l7iqrgsqL3cHqcpKl8CKilxrKj7eHTzDwtyryhdfuINo69au3hUrXNm+\nfd2BLTvbtXQKCuCss9wBb9Eid/DbvdudZNGpk0seW7fCb37j4hk3Dl5+2V0oW1rqlrnoIncQzc//\n/tHQw4Z93924caNLmOXlbn3Dh7sxtwMH4Ikn3Lacd55LGhER7qCalORamAkJbrsLCtx1U+vXuwNy\nQYHrWj1wwK2nTRuXQKOj3bJbt7rYy8tdMs7MdAmsvNwlvW++cckhJsZth+r33ZndurnPCxe6utu3\ndwf+rVtdItq1y/146NPHJauQEJcMiopc6y4+3u3PNm1cMh0wwP2IEHEJrSrpFBe77S4tdV20e/Z8\nf4p+fLxLOuXlbr+cc46ra/Fit10bNrjxxMsvh9/+tnH+Bi2BeHwTyB8X/pHC0kL+/JM/BzgqY0xN\nVYcbOabD1pEdPOgO6AUFrjV4zjmu9ZKe/v3JIMnJhy6Tk+MSe1WX4ZYtLnF06uQO3p984q7Rqmrl\ntWnj6ty40SXq7GyXKFavdomhVSuXfAYOdNubl+fOmmzTxt1rLy7OJYuCAtcKi4x06+3Rw10vFhHh\nku+ePa6OzZtd0h4/vnH2kSUQj28CeXPNm7y6+lXevfLdAEdljDHNV2MkkBZ3I40TOp7Asp3LOFB2\nINChGGNMi9YiE8io5FHc8fEdgQ7FGGNatBbXhQXu4VL9/t6PfXfvQ/zd2WqMMUHIurDq0Kl9J6Lb\nRLN53+ZAh2KMMS1Wi0wgAKcmnMqyncsCHYYxxrRYLTaBnNb1NJbvXB7oMIwxpsVq0QlkwbYF+I6N\nrM5ezaLMRQGMyhhjWo4Wm0BG9xxNWWUZr695vXrac98+x8NfPhzAqIwxpuVosQkkNCSUf1zwD343\n73cUlBQAsCp7FV9mfEmlVgY4OmOMCX4tNoEAnJl0JqN7jmZa6jQAvsv6jhAJYXX26gBHZowxwa/Z\nPpGwsTxy7iOc+vSpDOgwgLKKMn464Kf8d8t/eWnlS5zW9TSuHHxloEM0xpig1CIvJKzp6+1fk/Ji\nCsMTh/PIuY9w7svnUlhayJlJZzJ/8vwmjNQYY5qHxriQsMW3QACGdxvOM+OeoaCkgFO7nspLP32J\nsJAwLn/zcnIP5NKhXTO6Sb8xxgSJ46IFUpcr3ryCn/T8CZOHTOaNNW8wuudoukd390OExhjTvNjt\n3D0NTSDzNs/jd/N+R1irMNqGtiUjP4Mvf/4lSdFJZORnENMmhqjwKD9EbIwxgRUU98ISkbEikiYi\nG0Tk7jrKPCEiG0VkhYgM9aZ1E5HPRGSNiKwSkdsaO7af9PoJJRUlJEUn8cX1X/CLU37BzR/ezIcb\nPuTEf51I0t+S+GTTJ429WmOMaRH82gIRkRBgAzAG2AksBSaoappPmfOBW1X1QhEZDjyuqiNEpAvQ\nRVVXiEgEsBwY77usTx0NaoEAbN67mU7tOxEZHklpRSnnv3I+a3PW8tplrxEWEsb4OeNJuzXNxkmM\nMS1Ks+/CEpERwDRVPd/7PAVQVZ3hU2YmsEBVX/c+rwNSVDWrRl3vAX9X1R+cNnUsCeRIbvj3DSRG\nJTIyaSRZRVmEhYQxoMMAhnQZQkFJAe3C2tEqpNWRKzLGmGYkGM7CSgQyfT5vB4YdocwOb1p1AhGR\nHsBQ4Gt/BHk4d/zoDobMHMLwxOH0iOlBeWU5qdtSGdFtBJ9u+ZSI1hHcn3I/N59+MwBrstfwPx/+\nDynJKTxw9gOHPI9kfe56+sX3q56mquwr3kdc27hD1qmq9hwTY0yz1+xP4/W6r94CblfVwqZe/+BO\ng8n9fS6xbWOrp83fMp9XV71K7u9zydyfyTkvncPHmz7mQNkBVmat5A8j/8Cz3z5LYWkhY/uMZWTy\nSP659J/c9eldTDlzClPPmsp9C+5jXe46UrelknpdKm+seYMZP5nBU8ufYvbK2aRel0rrVq2benON\nMabe/J1AdgBJPp+7edNqluleWxkRCcUlj5dU9d+HW9H06dOr36ekpJCSktLQmH/AN3kAjOk1hjG9\nxgAwoMMA5k+ez1fbv6JT+06c2OlEEqMSmTxkMr98/5fcOe9O2oS2Yc+BPaz4nxVMeGsC76a9S+/Y\n3lzU7yKiwqMYM3sMB8sOEt82nr8u/isDOgzgV+//ivtT7ie6TTQxbWKq171p7yY6te90xLPDisuL\naRPapt7bWFJeQlirMEKkRd/dxpjjVmpqKqmpqY1ap7/HQFoB63GD6LuAJcBEVV3nU+YC4BZvEH0E\n8JiqjvDmzQZyVfXOI6zHb2Mgx6qkvIQb5t7Ar4f9muHdhrPv4D4eWfwIU86cQkTrCNZkr+GX7/+S\nu398N3d8cgd/HP1HLuh7Abf95zY+3fIphaWFnNv7XMb0HEMracW9n91LcnQy4aHhRIdHM+mkSTz+\n9eP84aw/cHH/i9l3cB8PffEQM5fN5NNJn/Kj7j8C4Ntd37IocxEX9ruQt9a+xSkJp9A/vj+tQloR\n2TqSEbNG0Ca0DT/u/mNuOf0W+sb3PWQ7sgqziAyPpF1YO7/tq0qtJPdALp3ad/LbOowxTrMfRAd3\nGi/wOO6U4Vmq+mcRuRE3mP60V+ZJYCxQBFynqt+KyI+BhcAqQL3Xvar6cS3raLYJ5FiVlJdw/+f3\ns6NgB7kHcrnihCvYXbibHjE9KK0o5Y8L/8iEwRN49ptniQqPYmfBTsYPGM/Y3mO56cObGNJlCJNP\nmszUBVOJDI8kqzCLiYMn8m7au9XjLF0iunBS55MY3388y3Yu45VVr3Bxv4vJL8nn5tNvpri8mItf\nu5gKraBHTA/+NPpPbNy7kf857X/4fNvnPL/ieaaNmsZJnU8CIC03jU7tO3Gw/CB/+b+/cHH/i/lJ\nr5/8YNu27NvCZW9cxhNjn+CPX/yRvOI81ueuZ83Na0iMSqS4vJi84jy6RHSpXiY9L50KraBXbK96\n7b/SitIGdwX+d8t/GdFtBBGtI+pVvlIrEaRRx6+25W1j5e6VjB8wvtHqNAaCJIE0hZacQOqrrKKM\n1dmr6Rvft/qAV1pRygsrXmDu+rn8ZsRvOL3r6WQVZdEvvh/Ldy6nuLyYyPBIsouyGZU8irBWYYAb\n41mXu45KrWTG/80g90AuH0z8gDG9xvDCihe4b8F9jOg2gi8zvqRCK5h00iTeXvc2pRWlhIa4XlFV\npbSilMsGXsbb695mVI9RdI/qzqurXqVrZFdSeqTw2dbPEBFWZa3iNyN+w/DE4XyX9R2vrHqFH3X/\nEQUlBXy29TMu7n8x4aHhJEQk8Ow3z6Iolw+8nLW5azkt4TS+yPiCTXs3ceWgK1mbu5ZhXYfRJrQN\nGfszeHvt2/zhrD/wZcaXtA1ry9jeY4luE82ynctYnb2a2T+dzYcbPqRrZFd2FuzkYPlBxvcfz/TU\n6cz+bjZjeo7hxUteZOqCqfSL78dF/S6iY7uOhIaEsiZnDeGtwimpKKFtaFuueucqisuLee/K99i8\nbzP5xflc0PcCdhTsIKJ1BD1iepBXnMe8zfMYljiMblHd+HTzp8S1jWNY4jDW71nP4szF1S3VXrG9\nGD9nPGty1rDulnUkRSf94HvPKcohdVsqhaWFtG/dntE9R1efcl6plXy6+VPW71nP5CGTCZEQlu9c\nzhndzyA8NBxV5Z117/D+hvcBOKfXOVx14lWICKuzV5MQkUDugVwSIhMO6TLNK84jKjyqUZJlQUkB\nFVpxSDdtc5RTlMPuwt2c0PGEgJ51WamVjdbNbAnEYwnEf1SV/JL8Wv+Dr8paxc6CnZzX5zxUla15\nWymvLKdvXF/mb51PTJsYTut6GquyVrEmZw1rstcwrv84SspL+DLjSzq278jVJ17NM988wy2n30Kr\nkFaUV5bz1fav+Hfav1mTs4ZHzn2EJTuWUFZZRkZ+BqN7jmZI5yHcO/9eesX24qsdXzG291jG9BrD\nP5b8g2GJw1ibs5YQCSE0JJShXYbyr2X/4pqTrmHfwX18mfklm/Zu4kDZAU7oeALvpb3HqORRrMtd\nR3zbePrG9+WDDR8wts9Ynhn3DDfMvYFPN3/Kxf0vpk1oG+Ztnkd+ST6VWkmfuD4UlxdTUFJApVby\n0OiHKCgt4JHFj5AYmUhCZAKfbf2M6PBoisuL6dCuA7sKdzE8cTjLdi6jQisY0GEAuQdyiW0Ty/b9\n2xnRbQTf7PqGju07kp6XzsX9LyYpOokXV75It6hulJS7C18LSwspLC1kbc5azkw6k7i2cewr3sfn\n2z6nR0wPtuZtpV1YO7pGdqVvXF8+3PghoSGhJEcnszZnLf079Ce+bTx7Du7htmG3ESIhPLn0STbv\n3cypXU/l213fEhke6RJTWHsu6ncRq7JX0Tu2N2+ve5uyijJCJISxfcYCsGTHEvrF92N/yX5ahbQi\nNCSUs5LOIkRCOFh+kM/TP6e8spyk6CS6R3UnNCSUsJAwXlj5AmUVZZyScAqd2nfipwN+SkZ+BkO7\nDGXDng1UaiUvrHyBiNYRxLaJJbZtLH1i+xAaEsr2/dvZkreFbXnb6BrZlf0l++kW1Y3SilI6tutI\nen46ablp3HvmveQcyOHkLiezbOcyQiSE2LaxfLTxIy7seyEAK7NWkp6fzsTBE6nUSqLD3dhjeGg4\ny3Yu46+L/kpMmxhCQ0L5+ck/57lvn2N8//F0j+5OfnE+uQdyiW4Tzdqctfz5J3+mZ0xPUrelkpab\nRkFpAeWV5Zzb+1wy8zPJ3J9JZn4m/eL7kV2UTffo7hwsO0jvuN6c3OVk3t/wPu+se4eYNjH8dMBP\n6dS+E1v2bWFHwQ6W7FjCO1e+0yj/ty2BeCyBmKOhqlRoBaEhodW/6EorSquTTkl5Ca1bta76D8ay\nncs4JeGU6l+eqsqBsgO0b90ecBejLtmxhIknTvzBug6WHSQ0JBRFWZezjuSYZGLaxFBWUcbeg3vp\nHNGZA2UHeGfdO1zc/+JaT46o1EqW71xOUVkR4a3CycjPoF1YO6LCozih4wl0bN+xumxBSQHrcteR\nHJ1MXnFe9Wnj+cX5FJcX0zmiM6UVpXyR/gWZ+zO55qRrqluNlVpJXnEen27+lBM7n8janLUM6TyE\n/SX7WZi+kD5xfVi2cxk3nHIDndt3pqisiE82fUJZZRnDE4ezLW8bsW1jKa0oJaswi9XZq6v389g+\nY6tjz8jPoEIr2HdwH1cOvpKeMT2Zt3keWUVZzFk9hz5xfVixewUDOw4k90Aud51xFyLC3oN72Xdw\nH6uyV1FaUUr/+P4kRCYwsMNAMvIziAyPZFveNtqHtSe7KJseMT2IaB3B3f+9m6FdhrI6ezUjk0Zy\nsPwga3PWctnAy1iyYwmtQloR0TqCPnF9WJy5mNatWpNfkk9ecR7F5cX0ievDbcNvY1DHQXy65VNm\nLpvJlYOu5KvtX1FWWUZk60ii20Szq2AXHdp14JHFjxAeGk6Hdh04retptJJWlFW6HoKk6CSSopLo\nGtmVFVkr6Ny+M9v3bycqPIoNezawfNdyzul1DtecdA17D+7lvbT3yD2QS6/YXhSXF/OPC/5Bckxy\no/w/sATisQRijGkuDpQdYHfhbnrG9DzqLr6mvAbMEojHEogxxhydoLiZojHGmJbJEogxxpgGsQRi\njDGmQSyBGGOMaRBLIMYYYxrEEogxxpgGsQRijDGmQSyBGGOMaRBLIMYYYxrEEogxxpgGsQRijDGm\nQSyBGGOMaRBLIMYYYxrEEogxxpgGsQRijDGmQfyeQERkrIikicgGEbm7jjJPiMhGEVkhIkOPZllj\njDGB4dcEIiIhwJPAecAgYKKIDKhR5nygt6r2BW4EZtZ32ZYgNTU10CEcE4s/sCz+wAr2+I+Vv1sg\nw4CNqpquqmXAHGB8jTLjgdkAqvo1EC0ineu5bNAL9j9Aiz+wLP7ACvb4j5W/E0gikOnzebs3rT5l\n6rOsMcaYAGmOg+hN80R5Y4wxx0RU1X+Vi4wApqvqWO/zFEBVdYZPmZnAAlV93fucBowCeh5pWZ86\n/LcRxhjTQqnqMf1gD22sQOqwFOgjIsnALmACMLFGmbnALcDrXsLJU9UsEcmtx7LAse8EY4wxR8+v\nCURVK0TkVmAerrtslqquE5Eb3Wx9WlU/EpELRGQTUARcf7hl/RmvMcaY+vNrF5YxxpiWqzkOotdb\nMF5oKCLbRGSliHwrIku8abEiMk9E1ovIJyISHeg4q4jILBHJEpHvfKbVGa+I3ONdFLpORM4NTNTf\nqyP+aSKyXUS+8V5jfeY1m/hFpJuIfCYia0RklYjc5k0Piv1fS/y/9qYHy/4PF5Gvvf+rq0Rkmjc9\nWPZ/XfE33v5X1aB84ZLfJiAZCANWAAMCHVc94t4CxNaYNgO4y3t/N/DnQMfpE9uZwFDguyPFC5wA\nfIvrGu3hfT/SDOOfBtxZS9mBzSl+oAsw1HsfAawHBgTL/j9M/EGx/72Y2nn/tgK+wl2fFhT7/zDx\nN9r+D+YWSLBeaCj8sOU3HnjRe/8icEmTRnQYqvolsK/G5LrivRiYo6rlqroN2Ij7ngKmjvih9tPF\nx9OM4lfV3aq6wntfCKwDuhEk+7+O+Kuu5Wr2+x9AVQ94b8NxB1YlSPY/1Bk/NNL+D+YEEqwXGirw\nqYgsFZFfeNM6q2oWuP90QKeARVc/neqIt+Z3soPm+53c6t177VmfLohmG7+I9MC1pL6i7r+XYIj/\na29SUOx/EQkRkW+B3cCnqrqUINr/dcQPjbT/gzmBBKsfq+opwAXALSIyku9/FVQJtjMbgi3efwK9\nVHUo7j/WIwGO57BEJAJ4C7jd+yUfVH8vtcQfNPtfVStV9WRcy2+YiAwiiPZ/LfGfQCPu/2BOIDuA\nJJ/P3bxpzZqq7vL+zQHewzURs8Td/wsR6QJkBy7Ceqkr3h1Ad59yzfI7UdUc9Tp9gWf4vpne7OIX\nkVDcwfclVf23Nzlo9n9t8QfT/q+iqvuBVGAsQbT/q/jG35j7P5gTSPVFiiLSGneh4dwAx3RYItLO\n+zWGiLQHzgVW4eK+zit2LfDvWisIHOHQPtO64p0LTBCR1iLSE+gDLGmqIA/jkPi9//RVLgVWe++b\nY/zPAWtV9XGfacG0/38Qf7DsfxHpUNW9IyJtgXNw4zhBsf/riD+tUfd/IM8QaIQzDMbizuzYCEwJ\ndDz1iLcn7myxb3GJY4o3PQ74r7ct84CYQMfqE/OrwE6gBMjAXegZW1e8wD24szfWAec20/hnA995\n38V7uD7tZhc/8GOgwudv5hvvb77Ov5cgiT9Y9v+JXswrvHj/15seLPu/rvgbbf/bhYTGGGMaJJi7\nsIwxxgSQJRBjjDENYgnEGGNMg1gCMcYY0yCWQIwxxjSIJRBjjDENYgnEmAASkVEi8n6g4zCmISyB\nGBN4djGWCUqWQIypBxG52ns4zzci8i/vLqcFIvKoiKwWkU9FJN4rO1REFnt3O33b53YSvb1yK0Rk\nmXe7CIBIEXnTe4jPSwHbSGOOkiUQY45ARAYAVwJnqLuTciVwNdAOWKKqg4GFuAf1gHtGxO/V3e10\ntc/0V4C/e9PPAHZ504cCt+EeSNRbRM7w/1YZc+xCAx2AMUFgDHAKsFREBGgDZOESyRtemZeBt0Uk\nCohW9yArcMnkDe8mmomqOhdAVUsBXHUsUe8uzSKyAvc0uEVNsF3GHBNLIMYcmQAvqur/HjJRZGqN\ncod72tvhlPi8r8D+X5ogYV1YxhzZfOByEekIICKxIpKEe8705V6Zq4Ev1T13Ya+I/NibPgn4XN2D\nlDJFZLxXR2vvFtvGBC37pWPMEajqOhH5AzBPREKAUuBWoAj3lLepuC6tK71FrgWe8hLEFtwt5MEl\nk6dF5AGvjitqW53/tsSYxmW3czemgUSkQFUjAx2HMYFiXVjGNJz9+jLHNWuBGGOMaRBrgRhjjGkQ\nSyDGGGMaxBKIMcaYBrEEYowxpkEsgRhjjGkQSyDGGGMa5P8D4j37WCC6j8sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d77bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('binary crossentropy loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
