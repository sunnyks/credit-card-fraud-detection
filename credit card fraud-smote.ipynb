{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "FILE_NAME = 'creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Label  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "full_data = pd.read_csv(FILE_NAME)\n",
    "\n",
    "#rename the 'Class' column\n",
    "full_data.rename(columns = {'Class': 'Label'}, inplace = True)\n",
    "\n",
    "#let's take a peek\n",
    "print full_data.shape\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consists of 284807 instances of data with 29 total features with value counts of \n",
      "0    284315\n",
      "1       492\n",
      "Name: Label, dtype: int64\n",
      "Where 0 indicates a legitimate transaction and 1 indicates fraud\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "full_data = shuffle(full_data)\n",
    "\n",
    "# Seperate target labels\n",
    "labels = full_data['Label']\n",
    "\n",
    "times = full_data['Time']\n",
    "features = full_data.drop(['Time', 'Label'], axis=1)\n",
    "\n",
    "# Get some specifics on our dataset\n",
    "print \"Data consists of {} instances of data with {} total features with value counts of \\n{}\".format(\n",
    "    features.shape[0], features.shape[1], labels.value_counts())\n",
    "print \"Where 0 indicates a legitimate transaction and 1 indicates fraud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the amount spent\n",
    "features['normAmount'] = StandardScaler().fit_transform(features['Amount'].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235327</th>\n",
       "      <td>1.703300</td>\n",
       "      <td>-0.863266</td>\n",
       "      <td>0.104905</td>\n",
       "      <td>1.247179</td>\n",
       "      <td>-0.141436</td>\n",
       "      <td>2.679679</td>\n",
       "      <td>-1.520703</td>\n",
       "      <td>1.006440</td>\n",
       "      <td>1.831379</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381504</td>\n",
       "      <td>0.059823</td>\n",
       "      <td>0.558681</td>\n",
       "      <td>0.301081</td>\n",
       "      <td>-0.708090</td>\n",
       "      <td>-0.460214</td>\n",
       "      <td>-0.605743</td>\n",
       "      <td>0.138376</td>\n",
       "      <td>-0.039287</td>\n",
       "      <td>-0.265072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266360</th>\n",
       "      <td>1.964137</td>\n",
       "      <td>0.058253</td>\n",
       "      <td>-1.797970</td>\n",
       "      <td>0.581708</td>\n",
       "      <td>0.126463</td>\n",
       "      <td>-1.452885</td>\n",
       "      <td>0.293474</td>\n",
       "      <td>-0.357730</td>\n",
       "      <td>0.646047</td>\n",
       "      <td>-0.529761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149845</td>\n",
       "      <td>0.216898</td>\n",
       "      <td>0.691547</td>\n",
       "      <td>-0.048083</td>\n",
       "      <td>-0.095469</td>\n",
       "      <td>0.207807</td>\n",
       "      <td>-0.101422</td>\n",
       "      <td>-0.008903</td>\n",
       "      <td>-0.022834</td>\n",
       "      <td>-0.167318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35223</th>\n",
       "      <td>1.369382</td>\n",
       "      <td>-0.436701</td>\n",
       "      <td>-0.346418</td>\n",
       "      <td>-0.821032</td>\n",
       "      <td>-0.338874</td>\n",
       "      <td>-0.399493</td>\n",
       "      <td>-0.210304</td>\n",
       "      <td>-0.068217</td>\n",
       "      <td>-1.341384</td>\n",
       "      <td>0.871767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533924</td>\n",
       "      <td>-0.359787</td>\n",
       "      <td>-0.575358</td>\n",
       "      <td>-0.019121</td>\n",
       "      <td>-0.274605</td>\n",
       "      <td>0.358820</td>\n",
       "      <td>1.106498</td>\n",
       "      <td>-0.079808</td>\n",
       "      <td>-0.022458</td>\n",
       "      <td>-0.321245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152565</th>\n",
       "      <td>1.974238</td>\n",
       "      <td>-0.404622</td>\n",
       "      <td>0.217208</td>\n",
       "      <td>0.539687</td>\n",
       "      <td>-0.934828</td>\n",
       "      <td>-0.375689</td>\n",
       "      <td>-0.945837</td>\n",
       "      <td>-0.071938</td>\n",
       "      <td>2.622833</td>\n",
       "      <td>-0.401860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178805</td>\n",
       "      <td>-0.118053</td>\n",
       "      <td>0.070297</td>\n",
       "      <td>0.303308</td>\n",
       "      <td>-0.133564</td>\n",
       "      <td>-0.591890</td>\n",
       "      <td>0.447994</td>\n",
       "      <td>-0.037839</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>-0.289460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280472</th>\n",
       "      <td>1.713941</td>\n",
       "      <td>-1.532205</td>\n",
       "      <td>-1.721972</td>\n",
       "      <td>-0.184136</td>\n",
       "      <td>-0.715999</td>\n",
       "      <td>-0.587145</td>\n",
       "      <td>-0.178479</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.731916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283905</td>\n",
       "      <td>0.059015</td>\n",
       "      <td>0.165055</td>\n",
       "      <td>-0.128037</td>\n",
       "      <td>0.559865</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>-0.059637</td>\n",
       "      <td>-0.049411</td>\n",
       "      <td>-0.011774</td>\n",
       "      <td>0.626302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "235327  1.703300 -0.863266  0.104905  1.247179 -0.141436  2.679679 -1.520703   \n",
       "266360  1.964137  0.058253 -1.797970  0.581708  0.126463 -1.452885  0.293474   \n",
       "35223   1.369382 -0.436701 -0.346418 -0.821032 -0.338874 -0.399493 -0.210304   \n",
       "152565  1.974238 -0.404622  0.217208  0.539687 -0.934828 -0.375689 -0.945837   \n",
       "280472  1.713941 -1.532205 -1.721972 -0.184136 -0.715999 -0.587145 -0.178479   \n",
       "\n",
       "              V8        V9       V10     ...           V20       V21  \\\n",
       "235327  1.006440  1.831379 -0.026611     ...     -0.381504  0.059823   \n",
       "266360 -0.357730  0.646047 -0.529761     ...     -0.149845  0.216898   \n",
       "35223  -0.068217 -1.341384  0.871767     ...     -0.533924 -0.359787   \n",
       "152565 -0.071938  2.622833 -0.401860     ...     -0.178805 -0.118053   \n",
       "280472 -0.177906  0.032715  0.731916     ...     -0.283905  0.059015   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "235327  0.558681  0.301081 -0.708090 -0.460214 -0.605743  0.138376 -0.039287   \n",
       "266360  0.691547 -0.048083 -0.095469  0.207807 -0.101422 -0.008903 -0.022834   \n",
       "35223  -0.575358 -0.019121 -0.274605  0.358820  1.106498 -0.079808 -0.022458   \n",
       "152565  0.070297  0.303308 -0.133564 -0.591890  0.447994 -0.037839 -0.043660   \n",
       "280472  0.165055 -0.128037  0.559865  0.072100 -0.059637 -0.049411 -0.011774   \n",
       "\n",
       "        normAmount  \n",
       "235327   -0.265072  \n",
       "266360   -0.167318  \n",
       "35223    -0.321245  \n",
       "152565   -0.289460  \n",
       "280472    0.626302  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts = features['Amount']\n",
    "features = features.drop(['Amount'], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#?????????????????\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    213230\n",
      "1       375\n",
      "Name: Label, dtype: int64\n",
      "0    71085\n",
      "1      117\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# We're going to hold out a test set from oversampling to see how our model trained on the oversampled data does on the original data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = .25)\n",
    "\n",
    "print train_labels.value_counts()\n",
    "print test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# implement synthetic minority oversampling technique for a more balanced dataset to feed our model\n",
    "oversampler = SMOTE(random_state=331)\n",
    "os_features, os_labels = oversampler.fit_sample(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training instances of data: 319845\n",
      "training instances of fraud 159912\n",
      "testing instances of data: 106615\n",
      "testing instances of fraud: 53318\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(os_features, os_labels, test_size = .25)\n",
    "\n",
    "# Let's get an idea of what our new oversampled data looks like\n",
    "\n",
    "print 'training instances of data:' , len(y_train) \n",
    "print 'training instances of fraud' , np.count_nonzero(y_train)\n",
    "print 'testing instances of data:' , len(y_test) \n",
    "print 'testing instances of fraud:' , np.count_nonzero(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Random Forest Classifier:\n",
      "[ 0.99981707  0.99982411  0.99983818] 0.999826453198\n",
      "For K-Nearest Neighbors Classifier:\n",
      "[ 0.99896484  0.99868985  0.99888023] 0.998844970617\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "print \"For Random Forest Classifier:\"\n",
    "rfscores = cross_val_score(rf, os_features, os_labels, scoring = f1_scorer)\n",
    "print rfscores, rfscores.mean()\n",
    "\n",
    "print \"For K-Nearest Neighbors Classifier:\"\n",
    "knnscores = cross_val_score(knn, os_features, os_labels, scoring = f1_scorer)\n",
    "print knnscores, knnscores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for simple majority vote is  0.999135510488\n"
     ]
    }
   ],
   "source": [
    "#majority vote benchmark without oversampling\n",
    "majority_vote_predictions = np.zeros(features.shape[0])\n",
    "print \"f1 score for simple majority vote is \" , f1_score(labels, majority_vote_predictions, pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1, score=0.999866 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1, score=0.999831 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   51.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1, score=0.999845 -   0.5s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21, score=0.999676 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21, score=0.999627 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21, score=0.999789 -   0.1s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=9, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=9, score=0.999838 -   1.9s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=9, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=9, score=0.999824 -   1.5s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=9, score=0.999845 -   1.5s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=9, score=0.999859 -   1.5s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=13, score=0.999796 -   2.5s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=13, score=0.999817 -   2.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=13, score=0.999824 -   2.3s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17, score=0.999810 - 1.7min\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17, score=0.999838 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17, score=0.999859 -   1.9s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=21, score=0.999655 -   1.1s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=21, score=0.999733 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=21, score=0.999740 -   1.2s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=5, score=0.999838 -   1.7s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=5, score=0.999845 -   1.7s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=5, score=0.999845 -   1.5s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=1, score=0.999873 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=1, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=1, score=0.999831 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999852 -   1.2s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999845 -   1.3s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999838 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999845 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999838 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999824 -   1.0s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9, score=0.999838 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9, score=0.999831 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9, score=0.999796 -   0.1s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=5, score=0.999817 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=5, score=0.999852 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=5, score=0.999824 -   1.1s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25, score=0.999683 -   1.6s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25, score=0.999634 -   2.5s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25, score=0.999627 -   1.6s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999866 -   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999845 -   2.4s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999859 -   2.4s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1, score=0.999859 -   2.2s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1, score=0.999852 -   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1, score=0.999859 -   2.3s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999817 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999789 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999845 -   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999817 -   1.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999824 -   1.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999845 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=5, score=0.999838 -   1.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=5, score=0.999845 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=5, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1, score=0.999873 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1, score=0.999810 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1, score=0.999845 -   1.2s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13, score=0.999845 -   1.4s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13, score=0.999817 -   1.5s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13, score=0.999859 -   1.4s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1, score=0.999852 -   1.9s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1, score=0.999838 -   1.9s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1, score=0.999852 -   1.8s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=17, score=0.999669 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=17, score=0.999719 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=17, score=0.999719 -   0.1s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999754 -   1.1s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999796 -   1.1s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999768 -   1.1s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=17, score=0.999754 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=17, score=0.999824 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=17, score=0.999775 -   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 439.6min finished\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators' : np.arange(10, 110, 15),\n",
    "                'min_samples_split': np.arange(2, 8, 2),\n",
    "                'max_features': np.arange(1, 29, 4),\n",
    "                'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "rf_tune = RandomizedSearchCV(rf, rf_params, n_iter = 25, scoring = f1_scorer, verbose = 3)\n",
    "\n",
    "rf_tune = rf_tune.fit(os_features, os_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False) \n",
      "f1 score: 0.999856941492\n"
     ]
    }
   ],
   "source": [
    "print rf_tune.best_estimator_ , '\\nf1 score:' , rf_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.14748263, -0.41471575, -0.27715551, -0.62149895,  0.361638  ,\n",
       "         0.04151148],\n",
       "       [-0.78123839, -0.96877863, -0.57963475, -1.3623686 ,  0.11400963,\n",
       "        -0.3206355 ],\n",
       "       [-6.27852821,  3.01893411, -6.60179368, -7.68470916, -2.57070958,\n",
       "         0.918958  ],\n",
       "       ..., \n",
       "       [ 0.1026754 ,  1.05104284,  1.64328036, -0.30613186,  0.36443625,\n",
       "         0.14194996],\n",
       "       [ 0.03093379,  0.49913424, -0.08120353,  0.76550463,  0.39444543,\n",
       "         2.08177092],\n",
       "       [-0.31799603, -0.60027605,  0.63097503, -0.58918361, -0.18149779,\n",
       "        -0.32864283]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft = rf_tune.best_estimator_\n",
    "rft.fit_transform(X_train, y_train)\n",
    "\n",
    "rfu = RandomForestClassifier()\n",
    "rfu.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 testing score for tuned random forest on oversampled data is  0.999849875209\n",
      "f1 testing score for random forest on oversampled data is  0.999812344011\n",
      "f1 testing score for tuned random forest on original test data is  0.999746789146\n"
     ]
    }
   ],
   "source": [
    "# Check performances of tuned and untuned models \n",
    "# Add check on original data before oversampling\n",
    "print \"f1 testing score for tuned random forest on oversampled data is \", f1_score(y_test, rft.predict(X_test), pos_label = 0)\n",
    "print \"f1 testing score for random forest on oversampled data is \" , f1_score(y_test, rfu.predict(X_test), pos_label = 0)\n",
    "\n",
    "print \"f1 testing score for tuned random forest on original test data is \", f1_score(test_labels, rft.predict(test_features), pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53281,    16],\n",
       "       [    0, 53318]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rft.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now let's try a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a model\n",
    "\n",
    "# needs more tuning\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim = X_train.shape[1], activation = 'tanh', init = 'lecun_uniform', W_regularizer = l2(.01)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(12, activation = 'tanh', init = 'lecun_uniform', W_regularizer = l2(.001)))\n",
    "model.add(Dense(4, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "sgd = SGD(lr = .16, momentum = .87, decay = .002)\n",
    "\n",
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 255876 samples, validate on 63969 samples\n",
      "Epoch 1/331\n",
      "1s - loss: 0.1859 - fmeasure: 0.9514 - val_loss: 0.0735 - val_fmeasure: 0.9679\n",
      "Epoch 2/331\n",
      "1s - loss: 0.0997 - fmeasure: 0.9735 - val_loss: 0.0608 - val_fmeasure: 0.9774\n",
      "Epoch 3/331\n",
      "1s - loss: 0.0861 - fmeasure: 0.9799 - val_loss: 0.0461 - val_fmeasure: 0.9847\n",
      "Epoch 4/331\n",
      "1s - loss: 0.0852 - fmeasure: 0.9828 - val_loss: 0.0450 - val_fmeasure: 0.9833\n",
      "Epoch 5/331\n",
      "1s - loss: 0.0744 - fmeasure: 0.9850 - val_loss: 0.0427 - val_fmeasure: 0.9860\n",
      "Epoch 6/331\n",
      "1s - loss: 0.0734 - fmeasure: 0.9849 - val_loss: 0.0347 - val_fmeasure: 0.9879\n",
      "Epoch 7/331\n",
      "1s - loss: 0.0672 - fmeasure: 0.9878 - val_loss: 0.0322 - val_fmeasure: 0.9888\n",
      "Epoch 8/331\n",
      "1s - loss: 0.0668 - fmeasure: 0.9874 - val_loss: 0.0468 - val_fmeasure: 0.9787\n",
      "Epoch 9/331\n",
      "1s - loss: 0.0653 - fmeasure: 0.9880 - val_loss: 0.0325 - val_fmeasure: 0.9904\n",
      "Epoch 10/331\n",
      "1s - loss: 0.0636 - fmeasure: 0.9886 - val_loss: 0.0338 - val_fmeasure: 0.9865\n",
      "Epoch 11/331\n",
      "1s - loss: 0.0637 - fmeasure: 0.9884 - val_loss: 0.0289 - val_fmeasure: 0.9901\n",
      "Epoch 12/331\n",
      "1s - loss: 0.0629 - fmeasure: 0.9887 - val_loss: 0.0300 - val_fmeasure: 0.9895\n",
      "Epoch 13/331\n",
      "1s - loss: 0.0606 - fmeasure: 0.9895 - val_loss: 0.0303 - val_fmeasure: 0.9910\n",
      "Epoch 14/331\n",
      "1s - loss: 0.0600 - fmeasure: 0.9897 - val_loss: 0.0279 - val_fmeasure: 0.9910\n",
      "Epoch 15/331\n",
      "1s - loss: 0.0591 - fmeasure: 0.9899 - val_loss: 0.0275 - val_fmeasure: 0.9909\n",
      "Epoch 16/331\n",
      "1s - loss: 0.0574 - fmeasure: 0.9907 - val_loss: 0.0281 - val_fmeasure: 0.9904\n",
      "Epoch 17/331\n",
      "1s - loss: 0.0583 - fmeasure: 0.9901 - val_loss: 0.0240 - val_fmeasure: 0.9933\n",
      "Epoch 18/331\n",
      "1s - loss: 0.0561 - fmeasure: 0.9908 - val_loss: 0.0290 - val_fmeasure: 0.9907\n",
      "Epoch 19/331\n",
      "1s - loss: 0.0560 - fmeasure: 0.9908 - val_loss: 0.0247 - val_fmeasure: 0.9927\n",
      "Epoch 20/331\n",
      "1s - loss: 0.0550 - fmeasure: 0.9911 - val_loss: 0.0266 - val_fmeasure: 0.9924\n",
      "Epoch 21/331\n",
      "1s - loss: 0.0553 - fmeasure: 0.9910 - val_loss: 0.0238 - val_fmeasure: 0.9934\n",
      "Epoch 22/331\n",
      "1s - loss: 0.0535 - fmeasure: 0.9919 - val_loss: 0.0231 - val_fmeasure: 0.9942\n",
      "Epoch 23/331\n",
      "1s - loss: 0.0538 - fmeasure: 0.9914 - val_loss: 0.0236 - val_fmeasure: 0.9935\n",
      "Epoch 24/331\n",
      "1s - loss: 0.0538 - fmeasure: 0.9915 - val_loss: 0.0241 - val_fmeasure: 0.9929\n",
      "Epoch 25/331\n",
      "1s - loss: 0.0529 - fmeasure: 0.9919 - val_loss: 0.0254 - val_fmeasure: 0.9917\n",
      "Epoch 26/331\n",
      "1s - loss: 0.0544 - fmeasure: 0.9912 - val_loss: 0.0218 - val_fmeasure: 0.9936\n",
      "Epoch 27/331\n",
      "1s - loss: 0.0523 - fmeasure: 0.9922 - val_loss: 0.0239 - val_fmeasure: 0.9935\n",
      "Epoch 28/331\n",
      "1s - loss: 0.0514 - fmeasure: 0.9923 - val_loss: 0.0231 - val_fmeasure: 0.9938\n",
      "Epoch 29/331\n",
      "1s - loss: 0.0513 - fmeasure: 0.9923 - val_loss: 0.0216 - val_fmeasure: 0.9936\n",
      "Epoch 30/331\n",
      "1s - loss: 0.0511 - fmeasure: 0.9924 - val_loss: 0.0217 - val_fmeasure: 0.9934\n",
      "Epoch 31/331\n",
      "1s - loss: 0.0506 - fmeasure: 0.9927 - val_loss: 0.0222 - val_fmeasure: 0.9934\n",
      "Epoch 32/331\n",
      "1s - loss: 0.0503 - fmeasure: 0.9927 - val_loss: 0.0224 - val_fmeasure: 0.9929\n",
      "Epoch 33/331\n",
      "1s - loss: 0.0505 - fmeasure: 0.9923 - val_loss: 0.0215 - val_fmeasure: 0.9938\n",
      "Epoch 34/331\n",
      "1s - loss: 0.0497 - fmeasure: 0.9927 - val_loss: 0.0240 - val_fmeasure: 0.9924\n",
      "Epoch 35/331\n",
      "1s - loss: 0.0510 - fmeasure: 0.9919 - val_loss: 0.0305 - val_fmeasure: 0.9883\n",
      "Epoch 36/331\n",
      "1s - loss: 0.0499 - fmeasure: 0.9926 - val_loss: 0.0207 - val_fmeasure: 0.9940\n",
      "Epoch 37/331\n",
      "1s - loss: 0.0492 - fmeasure: 0.9929 - val_loss: 0.0217 - val_fmeasure: 0.9936\n",
      "Epoch 38/331\n",
      "1s - loss: 0.0493 - fmeasure: 0.9929 - val_loss: 0.0257 - val_fmeasure: 0.9916\n",
      "Epoch 39/331\n",
      "1s - loss: 0.0494 - fmeasure: 0.9926 - val_loss: 0.0243 - val_fmeasure: 0.9922\n",
      "Epoch 40/331\n",
      "1s - loss: 0.0491 - fmeasure: 0.9930 - val_loss: 0.0210 - val_fmeasure: 0.9942\n",
      "Epoch 41/331\n",
      "1s - loss: 0.0490 - fmeasure: 0.9928 - val_loss: 0.0203 - val_fmeasure: 0.9944\n",
      "Epoch 42/331\n",
      "1s - loss: 0.0482 - fmeasure: 0.9932 - val_loss: 0.0205 - val_fmeasure: 0.9942\n",
      "Epoch 43/331\n",
      "1s - loss: 0.0485 - fmeasure: 0.9931 - val_loss: 0.0208 - val_fmeasure: 0.9942\n",
      "Epoch 44/331\n",
      "1s - loss: 0.0477 - fmeasure: 0.9933 - val_loss: 0.0205 - val_fmeasure: 0.9945\n",
      "Epoch 45/331\n",
      "1s - loss: 0.0480 - fmeasure: 0.9932 - val_loss: 0.0217 - val_fmeasure: 0.9934\n",
      "Epoch 46/331\n",
      "1s - loss: 0.0479 - fmeasure: 0.9933 - val_loss: 0.0224 - val_fmeasure: 0.9931\n",
      "Epoch 47/331\n",
      "1s - loss: 0.0480 - fmeasure: 0.9930 - val_loss: 0.0208 - val_fmeasure: 0.9932\n",
      "Epoch 48/331\n",
      "1s - loss: 0.0474 - fmeasure: 0.9935 - val_loss: 0.0201 - val_fmeasure: 0.9939\n",
      "Epoch 49/331\n",
      "1s - loss: 0.0466 - fmeasure: 0.9939 - val_loss: 0.0220 - val_fmeasure: 0.9940\n",
      "Epoch 50/331\n",
      "1s - loss: 0.0469 - fmeasure: 0.9934 - val_loss: 0.0205 - val_fmeasure: 0.9944\n",
      "Epoch 51/331\n",
      "1s - loss: 0.0468 - fmeasure: 0.9934 - val_loss: 0.0233 - val_fmeasure: 0.9923\n",
      "Epoch 52/331\n",
      "1s - loss: 0.0470 - fmeasure: 0.9934 - val_loss: 0.0194 - val_fmeasure: 0.9941\n",
      "Epoch 53/331\n",
      "1s - loss: 0.0472 - fmeasure: 0.9933 - val_loss: 0.0209 - val_fmeasure: 0.9943\n",
      "Epoch 54/331\n",
      "1s - loss: 0.0463 - fmeasure: 0.9938 - val_loss: 0.0254 - val_fmeasure: 0.9913\n",
      "Epoch 55/331\n",
      "1s - loss: 0.0468 - fmeasure: 0.9934 - val_loss: 0.0208 - val_fmeasure: 0.9937\n",
      "Epoch 56/331\n",
      "1s - loss: 0.0462 - fmeasure: 0.9939 - val_loss: 0.0190 - val_fmeasure: 0.9953\n",
      "Epoch 57/331\n",
      "1s - loss: 0.0468 - fmeasure: 0.9935 - val_loss: 0.0209 - val_fmeasure: 0.9940\n",
      "Epoch 58/331\n",
      "1s - loss: 0.0463 - fmeasure: 0.9938 - val_loss: 0.0211 - val_fmeasure: 0.9934\n",
      "Epoch 59/331\n",
      "1s - loss: 0.0468 - fmeasure: 0.9934 - val_loss: 0.0201 - val_fmeasure: 0.9948\n",
      "Epoch 60/331\n",
      "1s - loss: 0.0464 - fmeasure: 0.9935 - val_loss: 0.0213 - val_fmeasure: 0.9936\n",
      "Epoch 61/331\n",
      "1s - loss: 0.0459 - fmeasure: 0.9938 - val_loss: 0.0205 - val_fmeasure: 0.9936\n",
      "Epoch 62/331\n",
      "1s - loss: 0.0458 - fmeasure: 0.9938 - val_loss: 0.0209 - val_fmeasure: 0.9932\n",
      "Epoch 63/331\n",
      "1s - loss: 0.0461 - fmeasure: 0.9936 - val_loss: 0.0198 - val_fmeasure: 0.9949\n",
      "Epoch 64/331\n",
      "1s - loss: 0.0451 - fmeasure: 0.9941 - val_loss: 0.0198 - val_fmeasure: 0.9941\n",
      "Epoch 65/331\n",
      "1s - loss: 0.0453 - fmeasure: 0.9940 - val_loss: 0.0213 - val_fmeasure: 0.9938\n",
      "Epoch 66/331\n",
      "1s - loss: 0.0452 - fmeasure: 0.9939 - val_loss: 0.0201 - val_fmeasure: 0.9941\n",
      "Epoch 67/331\n",
      "1s - loss: 0.0452 - fmeasure: 0.9940 - val_loss: 0.0201 - val_fmeasure: 0.9949\n",
      "Epoch 68/331\n",
      "1s - loss: 0.0451 - fmeasure: 0.9939 - val_loss: 0.0194 - val_fmeasure: 0.9945\n",
      "Epoch 69/331\n",
      "1s - loss: 0.0448 - fmeasure: 0.9942 - val_loss: 0.0231 - val_fmeasure: 0.9927\n",
      "Epoch 70/331\n",
      "1s - loss: 0.0449 - fmeasure: 0.9937 - val_loss: 0.0208 - val_fmeasure: 0.9930\n",
      "Epoch 71/331\n",
      "1s - loss: 0.0448 - fmeasure: 0.9939 - val_loss: 0.0185 - val_fmeasure: 0.9949\n",
      "Epoch 72/331\n",
      "1s - loss: 0.0445 - fmeasure: 0.9942 - val_loss: 0.0186 - val_fmeasure: 0.9948\n",
      "Epoch 73/331\n",
      "1s - loss: 0.0444 - fmeasure: 0.9942 - val_loss: 0.0195 - val_fmeasure: 0.9946\n",
      "Epoch 74/331\n",
      "1s - loss: 0.0443 - fmeasure: 0.9942 - val_loss: 0.0197 - val_fmeasure: 0.9949\n",
      "Epoch 75/331\n",
      "1s - loss: 0.0445 - fmeasure: 0.9941 - val_loss: 0.0197 - val_fmeasure: 0.9951\n",
      "Epoch 76/331\n",
      "1s - loss: 0.0447 - fmeasure: 0.9939 - val_loss: 0.0198 - val_fmeasure: 0.9947\n",
      "Epoch 77/331\n",
      "1s - loss: 0.0446 - fmeasure: 0.9939 - val_loss: 0.0190 - val_fmeasure: 0.9946\n",
      "Epoch 78/331\n",
      "1s - loss: 0.0441 - fmeasure: 0.9943 - val_loss: 0.0201 - val_fmeasure: 0.9944\n",
      "Epoch 79/331\n",
      "1s - loss: 0.0439 - fmeasure: 0.9944 - val_loss: 0.0191 - val_fmeasure: 0.9953\n",
      "Epoch 80/331\n",
      "1s - loss: 0.0441 - fmeasure: 0.9941 - val_loss: 0.0193 - val_fmeasure: 0.9953\n",
      "Epoch 81/331\n",
      "1s - loss: 0.0436 - fmeasure: 0.9943 - val_loss: 0.0206 - val_fmeasure: 0.9931\n",
      "Epoch 82/331\n",
      "1s - loss: 0.0439 - fmeasure: 0.9943 - val_loss: 0.0188 - val_fmeasure: 0.9952\n",
      "Epoch 83/331\n",
      "1s - loss: 0.0441 - fmeasure: 0.9940 - val_loss: 0.0194 - val_fmeasure: 0.9946\n",
      "Epoch 84/331\n",
      "1s - loss: 0.0442 - fmeasure: 0.9942 - val_loss: 0.0187 - val_fmeasure: 0.9953\n",
      "Epoch 85/331\n",
      "1s - loss: 0.0437 - fmeasure: 0.9943 - val_loss: 0.0182 - val_fmeasure: 0.9951\n",
      "Epoch 86/331\n",
      "1s - loss: 0.0438 - fmeasure: 0.9941 - val_loss: 0.0209 - val_fmeasure: 0.9936\n",
      "Epoch 87/331\n",
      "1s - loss: 0.0437 - fmeasure: 0.9943 - val_loss: 0.0177 - val_fmeasure: 0.9949\n",
      "Epoch 88/331\n",
      "1s - loss: 0.0433 - fmeasure: 0.9946 - val_loss: 0.0180 - val_fmeasure: 0.9951\n",
      "Epoch 89/331\n",
      "1s - loss: 0.0431 - fmeasure: 0.9944 - val_loss: 0.0195 - val_fmeasure: 0.9950\n",
      "Epoch 90/331\n",
      "1s - loss: 0.0434 - fmeasure: 0.9942 - val_loss: 0.0210 - val_fmeasure: 0.9934\n",
      "Epoch 91/331\n",
      "1s - loss: 0.0435 - fmeasure: 0.9943 - val_loss: 0.0196 - val_fmeasure: 0.9947\n",
      "Epoch 92/331\n",
      "1s - loss: 0.0434 - fmeasure: 0.9943 - val_loss: 0.0191 - val_fmeasure: 0.9945\n",
      "Epoch 93/331\n",
      "1s - loss: 0.0433 - fmeasure: 0.9943 - val_loss: 0.0196 - val_fmeasure: 0.9939\n",
      "Epoch 94/331\n",
      "1s - loss: 0.0432 - fmeasure: 0.9944 - val_loss: 0.0197 - val_fmeasure: 0.9941\n",
      "Epoch 95/331\n",
      "1s - loss: 0.0432 - fmeasure: 0.9944 - val_loss: 0.0195 - val_fmeasure: 0.9946\n",
      "Epoch 96/331\n",
      "1s - loss: 0.0433 - fmeasure: 0.9944 - val_loss: 0.0189 - val_fmeasure: 0.9955\n",
      "Epoch 97/331\n",
      "1s - loss: 0.0437 - fmeasure: 0.9941 - val_loss: 0.0206 - val_fmeasure: 0.9939\n",
      "Epoch 98/331\n",
      "1s - loss: 0.0431 - fmeasure: 0.9943 - val_loss: 0.0191 - val_fmeasure: 0.9942\n",
      "Epoch 99/331\n",
      "1s - loss: 0.0427 - fmeasure: 0.9947 - val_loss: 0.0187 - val_fmeasure: 0.9945\n",
      "Epoch 100/331\n",
      "1s - loss: 0.0428 - fmeasure: 0.9945 - val_loss: 0.0179 - val_fmeasure: 0.9949\n",
      "Epoch 101/331\n",
      "1s - loss: 0.0426 - fmeasure: 0.9945 - val_loss: 0.0182 - val_fmeasure: 0.9955\n",
      "Epoch 102/331\n",
      "1s - loss: 0.0427 - fmeasure: 0.9945 - val_loss: 0.0183 - val_fmeasure: 0.9949\n",
      "Epoch 103/331\n",
      "1s - loss: 0.0425 - fmeasure: 0.9946 - val_loss: 0.0193 - val_fmeasure: 0.9949\n",
      "Epoch 104/331\n",
      "1s - loss: 0.0423 - fmeasure: 0.9947 - val_loss: 0.0185 - val_fmeasure: 0.9954\n",
      "Epoch 105/331\n",
      "1s - loss: 0.0425 - fmeasure: 0.9945 - val_loss: 0.0189 - val_fmeasure: 0.9942\n",
      "Epoch 106/331\n",
      "1s - loss: 0.0426 - fmeasure: 0.9945 - val_loss: 0.0178 - val_fmeasure: 0.9954\n",
      "Epoch 107/331\n",
      "1s - loss: 0.0426 - fmeasure: 0.9946 - val_loss: 0.0182 - val_fmeasure: 0.9947\n",
      "Epoch 108/331\n",
      "1s - loss: 0.0424 - fmeasure: 0.9944 - val_loss: 0.0190 - val_fmeasure: 0.9942\n",
      "Epoch 109/331\n",
      "1s - loss: 0.0421 - fmeasure: 0.9946 - val_loss: 0.0187 - val_fmeasure: 0.9950\n",
      "Epoch 110/331\n",
      "1s - loss: 0.0422 - fmeasure: 0.9947 - val_loss: 0.0188 - val_fmeasure: 0.9943\n",
      "Epoch 111/331\n",
      "1s - loss: 0.0423 - fmeasure: 0.9946 - val_loss: 0.0190 - val_fmeasure: 0.9945\n",
      "Epoch 112/331\n",
      "1s - loss: 0.0420 - fmeasure: 0.9947 - val_loss: 0.0184 - val_fmeasure: 0.9956\n",
      "Epoch 113/331\n",
      "1s - loss: 0.0420 - fmeasure: 0.9947 - val_loss: 0.0184 - val_fmeasure: 0.9950\n",
      "Epoch 114/331\n",
      "1s - loss: 0.0422 - fmeasure: 0.9946 - val_loss: 0.0180 - val_fmeasure: 0.9948\n",
      "Epoch 115/331\n",
      "1s - loss: 0.0423 - fmeasure: 0.9945 - val_loss: 0.0189 - val_fmeasure: 0.9945\n",
      "Epoch 116/331\n",
      "1s - loss: 0.0419 - fmeasure: 0.9947 - val_loss: 0.0201 - val_fmeasure: 0.9942\n",
      "Epoch 117/331\n",
      "1s - loss: 0.0418 - fmeasure: 0.9948 - val_loss: 0.0178 - val_fmeasure: 0.9948\n",
      "Epoch 118/331\n",
      "1s - loss: 0.0418 - fmeasure: 0.9948 - val_loss: 0.0177 - val_fmeasure: 0.9955\n",
      "Epoch 119/331\n",
      "1s - loss: 0.0418 - fmeasure: 0.9948 - val_loss: 0.0182 - val_fmeasure: 0.9951\n",
      "Epoch 120/331\n",
      "1s - loss: 0.0416 - fmeasure: 0.9949 - val_loss: 0.0203 - val_fmeasure: 0.9936\n",
      "Epoch 121/331\n",
      "1s - loss: 0.0423 - fmeasure: 0.9943 - val_loss: 0.0190 - val_fmeasure: 0.9939\n",
      "Epoch 122/331\n",
      "1s - loss: 0.0416 - fmeasure: 0.9949 - val_loss: 0.0191 - val_fmeasure: 0.9942\n",
      "Epoch 123/331\n",
      "1s - loss: 0.0416 - fmeasure: 0.9947 - val_loss: 0.0180 - val_fmeasure: 0.9953\n",
      "Epoch 124/331\n",
      "1s - loss: 0.0417 - fmeasure: 0.9948 - val_loss: 0.0223 - val_fmeasure: 0.9928\n",
      "Epoch 125/331\n",
      "1s - loss: 0.0416 - fmeasure: 0.9948 - val_loss: 0.0183 - val_fmeasure: 0.9951\n",
      "Epoch 126/331\n",
      "1s - loss: 0.0415 - fmeasure: 0.9948 - val_loss: 0.0179 - val_fmeasure: 0.9947\n",
      "Epoch 127/331\n",
      "1s - loss: 0.0416 - fmeasure: 0.9947 - val_loss: 0.0180 - val_fmeasure: 0.9952\n",
      "Epoch 128/331\n",
      "1s - loss: 0.0415 - fmeasure: 0.9948 - val_loss: 0.0181 - val_fmeasure: 0.9945\n",
      "Epoch 129/331\n",
      "1s - loss: 0.0414 - fmeasure: 0.9948 - val_loss: 0.0171 - val_fmeasure: 0.9949\n",
      "Epoch 130/331\n",
      "1s - loss: 0.0414 - fmeasure: 0.9948 - val_loss: 0.0210 - val_fmeasure: 0.9932\n",
      "Epoch 131/331\n",
      "1s - loss: 0.0413 - fmeasure: 0.9949 - val_loss: 0.0177 - val_fmeasure: 0.9959\n",
      "Epoch 132/331\n",
      "1s - loss: 0.0414 - fmeasure: 0.9948 - val_loss: 0.0193 - val_fmeasure: 0.9939\n",
      "Epoch 133/331\n",
      "1s - loss: 0.0414 - fmeasure: 0.9948 - val_loss: 0.0189 - val_fmeasure: 0.9952\n",
      "Epoch 134/331\n",
      "1s - loss: 0.0414 - fmeasure: 0.9948 - val_loss: 0.0183 - val_fmeasure: 0.9948\n",
      "Epoch 135/331\n",
      "1s - loss: 0.0410 - fmeasure: 0.9949 - val_loss: 0.0191 - val_fmeasure: 0.9942\n",
      "Epoch 136/331\n",
      "1s - loss: 0.0412 - fmeasure: 0.9948 - val_loss: 0.0188 - val_fmeasure: 0.9955\n",
      "Epoch 137/331\n",
      "1s - loss: 0.0410 - fmeasure: 0.9949 - val_loss: 0.0191 - val_fmeasure: 0.9941\n",
      "Epoch 138/331\n",
      "1s - loss: 0.0413 - fmeasure: 0.9947 - val_loss: 0.0178 - val_fmeasure: 0.9951\n",
      "Epoch 139/331\n",
      "1s - loss: 0.0412 - fmeasure: 0.9948 - val_loss: 0.0175 - val_fmeasure: 0.9951\n",
      "Epoch 140/331\n",
      "1s - loss: 0.0408 - fmeasure: 0.9950 - val_loss: 0.0191 - val_fmeasure: 0.9937\n",
      "Epoch 141/331\n",
      "1s - loss: 0.0413 - fmeasure: 0.9949 - val_loss: 0.0193 - val_fmeasure: 0.9953\n",
      "Epoch 142/331\n",
      "1s - loss: 0.0408 - fmeasure: 0.9950 - val_loss: 0.0176 - val_fmeasure: 0.9953\n",
      "Epoch 143/331\n",
      "1s - loss: 0.0408 - fmeasure: 0.9949 - val_loss: 0.0172 - val_fmeasure: 0.9955\n",
      "Epoch 144/331\n",
      "1s - loss: 0.0408 - fmeasure: 0.9949 - val_loss: 0.0175 - val_fmeasure: 0.9950\n",
      "Epoch 145/331\n",
      "1s - loss: 0.0409 - fmeasure: 0.9950 - val_loss: 0.0189 - val_fmeasure: 0.9940\n",
      "Epoch 146/331\n",
      "1s - loss: 0.0405 - fmeasure: 0.9951 - val_loss: 0.0183 - val_fmeasure: 0.9948\n",
      "Epoch 147/331\n",
      "1s - loss: 0.0410 - fmeasure: 0.9948 - val_loss: 0.0185 - val_fmeasure: 0.9957\n",
      "Epoch 148/331\n",
      "1s - loss: 0.0406 - fmeasure: 0.9951 - val_loss: 0.0181 - val_fmeasure: 0.9948\n",
      "Epoch 149/331\n",
      "1s - loss: 0.0408 - fmeasure: 0.9949 - val_loss: 0.0175 - val_fmeasure: 0.9953\n",
      "Epoch 150/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9952 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 151/331\n",
      "1s - loss: 0.0406 - fmeasure: 0.9949 - val_loss: 0.0171 - val_fmeasure: 0.9953\n",
      "Epoch 152/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9950 - val_loss: 0.0179 - val_fmeasure: 0.9954\n",
      "Epoch 153/331\n",
      "1s - loss: 0.0408 - fmeasure: 0.9949 - val_loss: 0.0173 - val_fmeasure: 0.9951\n",
      "Epoch 154/331\n",
      "1s - loss: 0.0406 - fmeasure: 0.9949 - val_loss: 0.0171 - val_fmeasure: 0.9960\n",
      "Epoch 155/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9950 - val_loss: 0.0185 - val_fmeasure: 0.9942\n",
      "Epoch 156/331\n",
      "1s - loss: 0.0408 - fmeasure: 0.9946 - val_loss: 0.0174 - val_fmeasure: 0.9951\n",
      "Epoch 157/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9950 - val_loss: 0.0177 - val_fmeasure: 0.9951\n",
      "Epoch 158/331\n",
      "1s - loss: 0.0405 - fmeasure: 0.9949 - val_loss: 0.0168 - val_fmeasure: 0.9954\n",
      "Epoch 159/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9950 - val_loss: 0.0172 - val_fmeasure: 0.9953\n",
      "Epoch 160/331\n",
      "1s - loss: 0.0403 - fmeasure: 0.9950 - val_loss: 0.0170 - val_fmeasure: 0.9954\n",
      "Epoch 161/331\n",
      "1s - loss: 0.0402 - fmeasure: 0.9952 - val_loss: 0.0176 - val_fmeasure: 0.9953\n",
      "Epoch 162/331\n",
      "1s - loss: 0.0402 - fmeasure: 0.9951 - val_loss: 0.0176 - val_fmeasure: 0.9956\n",
      "Epoch 163/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9951 - val_loss: 0.0171 - val_fmeasure: 0.9953\n",
      "Epoch 164/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9948 - val_loss: 0.0180 - val_fmeasure: 0.9944\n",
      "Epoch 165/331\n",
      "1s - loss: 0.0402 - fmeasure: 0.9951 - val_loss: 0.0171 - val_fmeasure: 0.9956\n",
      "Epoch 166/331\n",
      "1s - loss: 0.0402 - fmeasure: 0.9952 - val_loss: 0.0179 - val_fmeasure: 0.9945\n",
      "Epoch 167/331\n",
      "1s - loss: 0.0403 - fmeasure: 0.9950 - val_loss: 0.0178 - val_fmeasure: 0.9952\n",
      "Epoch 168/331\n",
      "1s - loss: 0.0401 - fmeasure: 0.9952 - val_loss: 0.0173 - val_fmeasure: 0.9951\n",
      "Epoch 169/331\n",
      "1s - loss: 0.0398 - fmeasure: 0.9952 - val_loss: 0.0172 - val_fmeasure: 0.9955\n",
      "Epoch 170/331\n",
      "1s - loss: 0.0402 - fmeasure: 0.9951 - val_loss: 0.0181 - val_fmeasure: 0.9953\n",
      "Epoch 171/331\n",
      "1s - loss: 0.0404 - fmeasure: 0.9951 - val_loss: 0.0189 - val_fmeasure: 0.9938\n",
      "Epoch 172/331\n",
      "1s - loss: 0.0401 - fmeasure: 0.9951 - val_loss: 0.0187 - val_fmeasure: 0.9941\n",
      "Epoch 173/331\n",
      "1s - loss: 0.0399 - fmeasure: 0.9951 - val_loss: 0.0178 - val_fmeasure: 0.9951\n",
      "Epoch 174/331\n",
      "1s - loss: 0.0401 - fmeasure: 0.9951 - val_loss: 0.0190 - val_fmeasure: 0.9943\n",
      "Epoch 175/331\n",
      "1s - loss: 0.0400 - fmeasure: 0.9952 - val_loss: 0.0170 - val_fmeasure: 0.9955\n",
      "Epoch 176/331\n",
      "1s - loss: 0.0400 - fmeasure: 0.9951 - val_loss: 0.0174 - val_fmeasure: 0.9953\n",
      "Epoch 177/331\n",
      "1s - loss: 0.0398 - fmeasure: 0.9952 - val_loss: 0.0168 - val_fmeasure: 0.9954\n",
      "Epoch 178/331\n",
      "1s - loss: 0.0399 - fmeasure: 0.9951 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 179/331\n",
      "1s - loss: 0.0397 - fmeasure: 0.9952 - val_loss: 0.0170 - val_fmeasure: 0.9950\n",
      "Epoch 180/331\n",
      "1s - loss: 0.0400 - fmeasure: 0.9951 - val_loss: 0.0168 - val_fmeasure: 0.9956\n",
      "Epoch 181/331\n",
      "1s - loss: 0.0397 - fmeasure: 0.9952 - val_loss: 0.0178 - val_fmeasure: 0.9947\n",
      "Epoch 182/331\n",
      "1s - loss: 0.0396 - fmeasure: 0.9952 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 183/331\n",
      "1s - loss: 0.0397 - fmeasure: 0.9951 - val_loss: 0.0174 - val_fmeasure: 0.9950\n",
      "Epoch 184/331\n",
      "1s - loss: 0.0395 - fmeasure: 0.9953 - val_loss: 0.0174 - val_fmeasure: 0.9954\n",
      "Epoch 185/331\n",
      "1s - loss: 0.0398 - fmeasure: 0.9951 - val_loss: 0.0169 - val_fmeasure: 0.9955\n",
      "Epoch 186/331\n",
      "1s - loss: 0.0399 - fmeasure: 0.9951 - val_loss: 0.0170 - val_fmeasure: 0.9955\n",
      "Epoch 187/331\n",
      "1s - loss: 0.0396 - fmeasure: 0.9953 - val_loss: 0.0174 - val_fmeasure: 0.9956\n",
      "Epoch 188/331\n",
      "1s - loss: 0.0397 - fmeasure: 0.9951 - val_loss: 0.0173 - val_fmeasure: 0.9952\n",
      "Epoch 189/331\n",
      "1s - loss: 0.0397 - fmeasure: 0.9951 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 190/331\n",
      "1s - loss: 0.0398 - fmeasure: 0.9951 - val_loss: 0.0171 - val_fmeasure: 0.9947\n",
      "Epoch 191/331\n",
      "1s - loss: 0.0394 - fmeasure: 0.9951 - val_loss: 0.0177 - val_fmeasure: 0.9956\n",
      "Epoch 192/331\n",
      "1s - loss: 0.0396 - fmeasure: 0.9952 - val_loss: 0.0173 - val_fmeasure: 0.9960\n",
      "Epoch 193/331\n",
      "1s - loss: 0.0395 - fmeasure: 0.9952 - val_loss: 0.0182 - val_fmeasure: 0.9957\n",
      "Epoch 194/331\n",
      "1s - loss: 0.0397 - fmeasure: 0.9951 - val_loss: 0.0167 - val_fmeasure: 0.9954\n",
      "Epoch 195/331\n",
      "1s - loss: 0.0393 - fmeasure: 0.9952 - val_loss: 0.0171 - val_fmeasure: 0.9951\n",
      "Epoch 196/331\n",
      "1s - loss: 0.0398 - fmeasure: 0.9949 - val_loss: 0.0173 - val_fmeasure: 0.9951\n",
      "Epoch 197/331\n",
      "1s - loss: 0.0393 - fmeasure: 0.9953 - val_loss: 0.0183 - val_fmeasure: 0.9951\n",
      "Epoch 198/331\n",
      "1s - loss: 0.0395 - fmeasure: 0.9951 - val_loss: 0.0171 - val_fmeasure: 0.9958\n",
      "Epoch 199/331\n",
      "1s - loss: 0.0394 - fmeasure: 0.9952 - val_loss: 0.0172 - val_fmeasure: 0.9949\n",
      "Epoch 200/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9953 - val_loss: 0.0167 - val_fmeasure: 0.9954\n",
      "Epoch 201/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9953 - val_loss: 0.0166 - val_fmeasure: 0.9955\n",
      "Epoch 202/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9954 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 203/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9952 - val_loss: 0.0171 - val_fmeasure: 0.9955\n",
      "Epoch 204/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9953 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 205/331\n",
      "1s - loss: 0.0393 - fmeasure: 0.9952 - val_loss: 0.0170 - val_fmeasure: 0.9954\n",
      "Epoch 206/331\n",
      "1s - loss: 0.0390 - fmeasure: 0.9952 - val_loss: 0.0169 - val_fmeasure: 0.9958\n",
      "Epoch 207/331\n",
      "1s - loss: 0.0391 - fmeasure: 0.9952 - val_loss: 0.0172 - val_fmeasure: 0.9953\n",
      "Epoch 208/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9953 - val_loss: 0.0179 - val_fmeasure: 0.9954\n",
      "Epoch 209/331\n",
      "1s - loss: 0.0390 - fmeasure: 0.9953 - val_loss: 0.0169 - val_fmeasure: 0.9952\n",
      "Epoch 210/331\n",
      "1s - loss: 0.0390 - fmeasure: 0.9954 - val_loss: 0.0178 - val_fmeasure: 0.9943\n",
      "Epoch 211/331\n",
      "1s - loss: 0.0391 - fmeasure: 0.9952 - val_loss: 0.0170 - val_fmeasure: 0.9951\n",
      "Epoch 212/331\n",
      "1s - loss: 0.0389 - fmeasure: 0.9954 - val_loss: 0.0174 - val_fmeasure: 0.9950\n",
      "Epoch 213/331\n",
      "1s - loss: 0.0390 - fmeasure: 0.9952 - val_loss: 0.0170 - val_fmeasure: 0.9954\n",
      "Epoch 214/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9951 - val_loss: 0.0166 - val_fmeasure: 0.9957\n",
      "Epoch 215/331\n",
      "1s - loss: 0.0389 - fmeasure: 0.9954 - val_loss: 0.0168 - val_fmeasure: 0.9953\n",
      "Epoch 216/331\n",
      "1s - loss: 0.0390 - fmeasure: 0.9953 - val_loss: 0.0182 - val_fmeasure: 0.9948\n",
      "Epoch 217/331\n",
      "1s - loss: 0.0390 - fmeasure: 0.9954 - val_loss: 0.0175 - val_fmeasure: 0.9956\n",
      "Epoch 218/331\n",
      "1s - loss: 0.0389 - fmeasure: 0.9954 - val_loss: 0.0164 - val_fmeasure: 0.9957\n",
      "Epoch 219/331\n",
      "1s - loss: 0.0388 - fmeasure: 0.9954 - val_loss: 0.0173 - val_fmeasure: 0.9956\n",
      "Epoch 220/331\n",
      "1s - loss: 0.0392 - fmeasure: 0.9953 - val_loss: 0.0179 - val_fmeasure: 0.9954\n",
      "Epoch 221/331\n",
      "1s - loss: 0.0388 - fmeasure: 0.9953 - val_loss: 0.0170 - val_fmeasure: 0.9961\n",
      "Epoch 222/331\n",
      "1s - loss: 0.0388 - fmeasure: 0.9952 - val_loss: 0.0164 - val_fmeasure: 0.9954\n",
      "Epoch 223/331\n",
      "1s - loss: 0.0389 - fmeasure: 0.9953 - val_loss: 0.0166 - val_fmeasure: 0.9956\n",
      "Epoch 224/331\n",
      "1s - loss: 0.0387 - fmeasure: 0.9954 - val_loss: 0.0173 - val_fmeasure: 0.9948\n",
      "Epoch 225/331\n",
      "1s - loss: 0.0387 - fmeasure: 0.9954 - val_loss: 0.0168 - val_fmeasure: 0.9957\n",
      "Epoch 226/331\n",
      "1s - loss: 0.0386 - fmeasure: 0.9955 - val_loss: 0.0171 - val_fmeasure: 0.9949\n",
      "Epoch 227/331\n",
      "1s - loss: 0.0388 - fmeasure: 0.9951 - val_loss: 0.0171 - val_fmeasure: 0.9950\n",
      "Epoch 228/331\n",
      "1s - loss: 0.0388 - fmeasure: 0.9953 - val_loss: 0.0170 - val_fmeasure: 0.9952\n",
      "Epoch 229/331\n",
      "1s - loss: 0.0387 - fmeasure: 0.9955 - val_loss: 0.0167 - val_fmeasure: 0.9961\n",
      "Epoch 230/331\n",
      "1s - loss: 0.0387 - fmeasure: 0.9954 - val_loss: 0.0165 - val_fmeasure: 0.9953\n",
      "Epoch 231/331\n",
      "1s - loss: 0.0388 - fmeasure: 0.9953 - val_loss: 0.0166 - val_fmeasure: 0.9950\n",
      "Epoch 232/331\n",
      "1s - loss: 0.0386 - fmeasure: 0.9953 - val_loss: 0.0179 - val_fmeasure: 0.9947\n",
      "Epoch 233/331\n",
      "1s - loss: 0.0388 - fmeasure: 0.9953 - val_loss: 0.0167 - val_fmeasure: 0.9950\n",
      "Epoch 234/331\n",
      "1s - loss: 0.0387 - fmeasure: 0.9952 - val_loss: 0.0163 - val_fmeasure: 0.9956\n",
      "Epoch 235/331\n",
      "1s - loss: 0.0387 - fmeasure: 0.9953 - val_loss: 0.0177 - val_fmeasure: 0.9953\n",
      "Epoch 236/331\n",
      "1s - loss: 0.0385 - fmeasure: 0.9954 - val_loss: 0.0177 - val_fmeasure: 0.9946\n",
      "Epoch 237/331\n",
      "1s - loss: 0.0383 - fmeasure: 0.9955 - val_loss: 0.0166 - val_fmeasure: 0.9954\n",
      "Epoch 238/331\n",
      "1s - loss: 0.0385 - fmeasure: 0.9953 - val_loss: 0.0164 - val_fmeasure: 0.9960\n",
      "Epoch 239/331\n",
      "1s - loss: 0.0386 - fmeasure: 0.9953 - val_loss: 0.0168 - val_fmeasure: 0.9955\n",
      "Epoch 240/331\n",
      "1s - loss: 0.0385 - fmeasure: 0.9954 - val_loss: 0.0168 - val_fmeasure: 0.9958\n",
      "Epoch 241/331\n",
      "1s - loss: 0.0385 - fmeasure: 0.9954 - val_loss: 0.0163 - val_fmeasure: 0.9950\n",
      "Epoch 242/331\n",
      "1s - loss: 0.0384 - fmeasure: 0.9954 - val_loss: 0.0166 - val_fmeasure: 0.9958\n",
      "Epoch 243/331\n",
      "1s - loss: 0.0385 - fmeasure: 0.9954 - val_loss: 0.0166 - val_fmeasure: 0.9956\n",
      "Epoch 244/331\n",
      "1s - loss: 0.0382 - fmeasure: 0.9955 - val_loss: 0.0161 - val_fmeasure: 0.9953\n",
      "Epoch 245/331\n",
      "1s - loss: 0.0384 - fmeasure: 0.9953 - val_loss: 0.0172 - val_fmeasure: 0.9952\n",
      "Epoch 246/331\n",
      "1s - loss: 0.0382 - fmeasure: 0.9955 - val_loss: 0.0165 - val_fmeasure: 0.9955\n",
      "Epoch 247/331\n",
      "1s - loss: 0.0382 - fmeasure: 0.9954 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 248/331\n",
      "1s - loss: 0.0383 - fmeasure: 0.9954 - val_loss: 0.0169 - val_fmeasure: 0.9953\n",
      "Epoch 249/331\n",
      "1s - loss: 0.0383 - fmeasure: 0.9954 - val_loss: 0.0160 - val_fmeasure: 0.9960\n",
      "Epoch 250/331\n",
      "1s - loss: 0.0383 - fmeasure: 0.9955 - val_loss: 0.0180 - val_fmeasure: 0.9944\n",
      "Epoch 251/331\n",
      "1s - loss: 0.0381 - fmeasure: 0.9955 - val_loss: 0.0175 - val_fmeasure: 0.9949\n",
      "Epoch 252/331\n",
      "1s - loss: 0.0380 - fmeasure: 0.9955 - val_loss: 0.0169 - val_fmeasure: 0.9947\n",
      "Epoch 253/331\n",
      "1s - loss: 0.0380 - fmeasure: 0.9955 - val_loss: 0.0187 - val_fmeasure: 0.9938\n",
      "Epoch 254/331\n",
      "1s - loss: 0.0382 - fmeasure: 0.9954 - val_loss: 0.0174 - val_fmeasure: 0.9957\n",
      "Epoch 255/331\n",
      "1s - loss: 0.0380 - fmeasure: 0.9956 - val_loss: 0.0161 - val_fmeasure: 0.9960\n",
      "Epoch 256/331\n",
      "1s - loss: 0.0382 - fmeasure: 0.9952 - val_loss: 0.0161 - val_fmeasure: 0.9955\n",
      "Epoch 257/331\n",
      "1s - loss: 0.0378 - fmeasure: 0.9956 - val_loss: 0.0160 - val_fmeasure: 0.9959\n",
      "Epoch 258/331\n",
      "1s - loss: 0.0378 - fmeasure: 0.9956 - val_loss: 0.0167 - val_fmeasure: 0.9948\n",
      "Epoch 259/331\n",
      "1s - loss: 0.0378 - fmeasure: 0.9955 - val_loss: 0.0165 - val_fmeasure: 0.9949\n",
      "Epoch 260/331\n",
      "1s - loss: 0.0380 - fmeasure: 0.9954 - val_loss: 0.0159 - val_fmeasure: 0.9957\n",
      "Epoch 261/331\n",
      "1s - loss: 0.0379 - fmeasure: 0.9956 - val_loss: 0.0158 - val_fmeasure: 0.9956\n",
      "Epoch 262/331\n",
      "1s - loss: 0.0377 - fmeasure: 0.9957 - val_loss: 0.0162 - val_fmeasure: 0.9957\n",
      "Epoch 263/331\n",
      "1s - loss: 0.0379 - fmeasure: 0.9956 - val_loss: 0.0167 - val_fmeasure: 0.9953\n",
      "Epoch 264/331\n",
      "1s - loss: 0.0379 - fmeasure: 0.9956 - val_loss: 0.0162 - val_fmeasure: 0.9952\n",
      "Epoch 265/331\n",
      "1s - loss: 0.0379 - fmeasure: 0.9954 - val_loss: 0.0159 - val_fmeasure: 0.9957\n",
      "Epoch 266/331\n",
      "1s - loss: 0.0379 - fmeasure: 0.9954 - val_loss: 0.0166 - val_fmeasure: 0.9955\n",
      "Epoch 267/331\n",
      "1s - loss: 0.0378 - fmeasure: 0.9956 - val_loss: 0.0161 - val_fmeasure: 0.9957\n",
      "Epoch 268/331\n",
      "1s - loss: 0.0378 - fmeasure: 0.9956 - val_loss: 0.0160 - val_fmeasure: 0.9959\n",
      "Epoch 269/331\n",
      "1s - loss: 0.0377 - fmeasure: 0.9956 - val_loss: 0.0181 - val_fmeasure: 0.9944\n",
      "Epoch 270/331\n",
      "1s - loss: 0.0377 - fmeasure: 0.9955 - val_loss: 0.0163 - val_fmeasure: 0.9960\n",
      "Epoch 271/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9957 - val_loss: 0.0163 - val_fmeasure: 0.9965\n",
      "Epoch 272/331\n",
      "1s - loss: 0.0377 - fmeasure: 0.9956 - val_loss: 0.0166 - val_fmeasure: 0.9951\n",
      "Epoch 273/331\n",
      "1s - loss: 0.0377 - fmeasure: 0.9955 - val_loss: 0.0179 - val_fmeasure: 0.9942\n",
      "Epoch 274/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9956 - val_loss: 0.0165 - val_fmeasure: 0.9954\n",
      "Epoch 275/331\n",
      "1s - loss: 0.0377 - fmeasure: 0.9955 - val_loss: 0.0155 - val_fmeasure: 0.9959\n",
      "Epoch 276/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9956 - val_loss: 0.0153 - val_fmeasure: 0.9961\n",
      "Epoch 277/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9955 - val_loss: 0.0153 - val_fmeasure: 0.9961\n",
      "Epoch 278/331\n",
      "1s - loss: 0.0375 - fmeasure: 0.9957 - val_loss: 0.0168 - val_fmeasure: 0.9948\n",
      "Epoch 279/331\n",
      "1s - loss: 0.0375 - fmeasure: 0.9955 - val_loss: 0.0174 - val_fmeasure: 0.9944\n",
      "Epoch 280/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9956 - val_loss: 0.0158 - val_fmeasure: 0.9956\n",
      "Epoch 281/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9956 - val_loss: 0.0168 - val_fmeasure: 0.9954\n",
      "Epoch 282/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9956 - val_loss: 0.0164 - val_fmeasure: 0.9956\n",
      "Epoch 283/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9956 - val_loss: 0.0174 - val_fmeasure: 0.9948\n",
      "Epoch 284/331\n",
      "1s - loss: 0.0375 - fmeasure: 0.9956 - val_loss: 0.0161 - val_fmeasure: 0.9953\n",
      "Epoch 285/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9956 - val_loss: 0.0162 - val_fmeasure: 0.9959\n",
      "Epoch 286/331\n",
      "1s - loss: 0.0375 - fmeasure: 0.9957 - val_loss: 0.0156 - val_fmeasure: 0.9962\n",
      "Epoch 287/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9957 - val_loss: 0.0157 - val_fmeasure: 0.9958\n",
      "Epoch 288/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9957 - val_loss: 0.0160 - val_fmeasure: 0.9958\n",
      "Epoch 289/331\n",
      "1s - loss: 0.0372 - fmeasure: 0.9957 - val_loss: 0.0165 - val_fmeasure: 0.9951\n",
      "Epoch 290/331\n",
      "1s - loss: 0.0375 - fmeasure: 0.9955 - val_loss: 0.0161 - val_fmeasure: 0.9957\n",
      "Epoch 291/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9956 - val_loss: 0.0160 - val_fmeasure: 0.9956\n",
      "Epoch 292/331\n",
      "1s - loss: 0.0375 - fmeasure: 0.9955 - val_loss: 0.0160 - val_fmeasure: 0.9962\n",
      "Epoch 293/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9956 - val_loss: 0.0160 - val_fmeasure: 0.9962\n",
      "Epoch 294/331\n",
      "1s - loss: 0.0376 - fmeasure: 0.9955 - val_loss: 0.0159 - val_fmeasure: 0.9957\n",
      "Epoch 295/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9958 - val_loss: 0.0158 - val_fmeasure: 0.9957\n",
      "Epoch 296/331\n",
      "1s - loss: 0.0372 - fmeasure: 0.9958 - val_loss: 0.0164 - val_fmeasure: 0.9955\n",
      "Epoch 297/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9956 - val_loss: 0.0167 - val_fmeasure: 0.9959\n",
      "Epoch 298/331\n",
      "1s - loss: 0.0375 - fmeasure: 0.9957 - val_loss: 0.0163 - val_fmeasure: 0.9958\n",
      "Epoch 299/331\n",
      "1s - loss: 0.0373 - fmeasure: 0.9956 - val_loss: 0.0163 - val_fmeasure: 0.9962\n",
      "Epoch 300/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9956 - val_loss: 0.0158 - val_fmeasure: 0.9960\n",
      "Epoch 301/331\n",
      "1s - loss: 0.0372 - fmeasure: 0.9956 - val_loss: 0.0156 - val_fmeasure: 0.9958\n",
      "Epoch 302/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9958 - val_loss: 0.0162 - val_fmeasure: 0.9954\n",
      "Epoch 303/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9957 - val_loss: 0.0158 - val_fmeasure: 0.9957\n",
      "Epoch 304/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9957 - val_loss: 0.0159 - val_fmeasure: 0.9958\n",
      "Epoch 305/331\n",
      "1s - loss: 0.0370 - fmeasure: 0.9957 - val_loss: 0.0157 - val_fmeasure: 0.9961\n",
      "Epoch 306/331\n",
      "1s - loss: 0.0372 - fmeasure: 0.9956 - val_loss: 0.0170 - val_fmeasure: 0.9962\n",
      "Epoch 307/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9956 - val_loss: 0.0165 - val_fmeasure: 0.9957\n",
      "Epoch 308/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9958 - val_loss: 0.0159 - val_fmeasure: 0.9966\n",
      "Epoch 309/331\n",
      "1s - loss: 0.0370 - fmeasure: 0.9957 - val_loss: 0.0168 - val_fmeasure: 0.9949\n",
      "Epoch 310/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9957 - val_loss: 0.0157 - val_fmeasure: 0.9957\n",
      "Epoch 311/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9956 - val_loss: 0.0157 - val_fmeasure: 0.9963\n",
      "Epoch 312/331\n",
      "1s - loss: 0.0372 - fmeasure: 0.9956 - val_loss: 0.0157 - val_fmeasure: 0.9958\n",
      "Epoch 313/331\n",
      "1s - loss: 0.0369 - fmeasure: 0.9957 - val_loss: 0.0164 - val_fmeasure: 0.9949\n",
      "Epoch 314/331\n",
      "1s - loss: 0.0368 - fmeasure: 0.9957 - val_loss: 0.0157 - val_fmeasure: 0.9963\n",
      "Epoch 315/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9957 - val_loss: 0.0154 - val_fmeasure: 0.9958\n",
      "Epoch 316/331\n",
      "1s - loss: 0.0369 - fmeasure: 0.9957 - val_loss: 0.0164 - val_fmeasure: 0.9958\n",
      "Epoch 317/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9957 - val_loss: 0.0154 - val_fmeasure: 0.9961\n",
      "Epoch 318/331\n",
      "1s - loss: 0.0369 - fmeasure: 0.9957 - val_loss: 0.0154 - val_fmeasure: 0.9960\n",
      "Epoch 319/331\n",
      "1s - loss: 0.0368 - fmeasure: 0.9957 - val_loss: 0.0159 - val_fmeasure: 0.9954\n",
      "Epoch 320/331\n",
      "1s - loss: 0.0368 - fmeasure: 0.9956 - val_loss: 0.0156 - val_fmeasure: 0.9963\n",
      "Epoch 321/331\n",
      "1s - loss: 0.0370 - fmeasure: 0.9957 - val_loss: 0.0167 - val_fmeasure: 0.9958\n",
      "Epoch 322/331\n",
      "1s - loss: 0.0374 - fmeasure: 0.9956 - val_loss: 0.0159 - val_fmeasure: 0.9961\n",
      "Epoch 323/331\n",
      "1s - loss: 0.0372 - fmeasure: 0.9956 - val_loss: 0.0154 - val_fmeasure: 0.9960\n",
      "Epoch 324/331\n",
      "1s - loss: 0.0368 - fmeasure: 0.9958 - val_loss: 0.0157 - val_fmeasure: 0.9964\n",
      "Epoch 325/331\n",
      "1s - loss: 0.0370 - fmeasure: 0.9956 - val_loss: 0.0162 - val_fmeasure: 0.9961\n",
      "Epoch 326/331\n",
      "1s - loss: 0.0370 - fmeasure: 0.9959 - val_loss: 0.0161 - val_fmeasure: 0.9961\n",
      "Epoch 327/331\n",
      "1s - loss: 0.0368 - fmeasure: 0.9957 - val_loss: 0.0161 - val_fmeasure: 0.9956\n",
      "Epoch 328/331\n",
      "1s - loss: 0.0368 - fmeasure: 0.9958 - val_loss: 0.0153 - val_fmeasure: 0.9960\n",
      "Epoch 329/331\n",
      "1s - loss: 0.0369 - fmeasure: 0.9956 - val_loss: 0.0157 - val_fmeasure: 0.9960\n",
      "Epoch 330/331\n",
      "1s - loss: 0.0371 - fmeasure: 0.9957 - val_loss: 0.0153 - val_fmeasure: 0.9965\n",
      "Epoch 331/331\n",
      "1s - loss: 0.0367 - fmeasure: 0.9958 - val_loss: 0.0158 - val_fmeasure: 0.9961\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, nb_epoch = 331, batch_size = 1000, verbose = 2, \n",
    "                    validation_split = .20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our neural network achieves an f1 score of  0.995486176415 on the oversampled testing data\n",
      "[[52930   367]\n",
      " [  113 53205]]\n",
      "Our neural network achieves an f1 score of 0.996952724913 on the original data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[70667,   418],\n",
       "       [   14,   103]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions[:] = predictions[:]>0.5\n",
    "\n",
    "print \"Our neural network achieves an f1 score of \" , f1_score(y_test, predictions, pos_label = 0) , \"on the oversampled testing data\"\n",
    "print confusion_matrix(y_test, predictions)\n",
    "\n",
    "opredictions = model.predict(test_features.values)\n",
    "opredictions[:] = opredictions[:]>0.5\n",
    "print \"Our neural network achieves an f1 score of\" , f1_score(test_labels.values, opredictions, pos_label = 0) , \"on the original testing data\"\n",
    "confusion_matrix(test_labels.values, opredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a160828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('binary crossentropy loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
