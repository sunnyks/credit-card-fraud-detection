{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "FILE_NAME = 'creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Label  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "full_data = pd.read_csv(FILE_NAME)\n",
    "\n",
    "#rename the 'Class' column\n",
    "full_data.rename(columns = {'Class': 'Label'}, inplace = True)\n",
    "\n",
    "#let's take a peek\n",
    "print full_data.shape\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#full_data.groupby('Label').hist(figsize = (20,20))\n",
    "#pd.scatter_matrix(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consists of 284807 instances of data with 29 total features with value counts of \n",
      "0    284315\n",
      "1       492\n",
      "Name: Label, dtype: int64\n",
      "Where 0 indicates a legitimate transaction and 1 indicates fraud\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "full_data = shuffle(full_data)\n",
    "\n",
    "labels = full_data['Label']\n",
    "times = full_data['Time']\n",
    "features = full_data.drop(['Time', 'Label'], axis=1)\n",
    "\n",
    "print \"Data consists of {} instances of data with {} total features with value counts of \\n{}\".format(\n",
    "    features.shape[0], features.shape[1], labels.value_counts())\n",
    "print \"Where 0 indicates a legitimate transaction and 1 indicates fraud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features['normAmount'] = StandardScaler().fit_transform(features['Amount'].reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88005</th>\n",
       "      <td>0.065203</td>\n",
       "      <td>0.560112</td>\n",
       "      <td>1.125474</td>\n",
       "      <td>0.540353</td>\n",
       "      <td>-0.536414</td>\n",
       "      <td>-0.320562</td>\n",
       "      <td>0.302465</td>\n",
       "      <td>0.057764</td>\n",
       "      <td>0.060539</td>\n",
       "      <td>0.017287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108628</td>\n",
       "      <td>-0.051882</td>\n",
       "      <td>-0.043935</td>\n",
       "      <td>0.279408</td>\n",
       "      <td>0.550676</td>\n",
       "      <td>-1.400744</td>\n",
       "      <td>0.109110</td>\n",
       "      <td>0.047491</td>\n",
       "      <td>0.154411</td>\n",
       "      <td>-0.237525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167431</th>\n",
       "      <td>-0.873838</td>\n",
       "      <td>0.474974</td>\n",
       "      <td>-1.415775</td>\n",
       "      <td>-0.484232</td>\n",
       "      <td>1.016395</td>\n",
       "      <td>-0.760008</td>\n",
       "      <td>1.889260</td>\n",
       "      <td>0.044478</td>\n",
       "      <td>-1.530959</td>\n",
       "      <td>-0.754338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271137</td>\n",
       "      <td>0.537073</td>\n",
       "      <td>1.012363</td>\n",
       "      <td>-0.061086</td>\n",
       "      <td>0.792703</td>\n",
       "      <td>1.023667</td>\n",
       "      <td>0.670202</td>\n",
       "      <td>-0.314630</td>\n",
       "      <td>-0.170169</td>\n",
       "      <td>0.422119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59392</th>\n",
       "      <td>1.279947</td>\n",
       "      <td>-0.846119</td>\n",
       "      <td>0.426752</td>\n",
       "      <td>-0.840060</td>\n",
       "      <td>-0.996439</td>\n",
       "      <td>0.027175</td>\n",
       "      <td>-0.972928</td>\n",
       "      <td>0.208130</td>\n",
       "      <td>-0.651566</td>\n",
       "      <td>0.785956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039822</td>\n",
       "      <td>0.406552</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>-0.170415</td>\n",
       "      <td>-0.302680</td>\n",
       "      <td>0.445933</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.015554</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>-0.192826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20131</th>\n",
       "      <td>1.225637</td>\n",
       "      <td>-0.649268</td>\n",
       "      <td>-0.119134</td>\n",
       "      <td>-0.513312</td>\n",
       "      <td>-0.797736</td>\n",
       "      <td>-0.561421</td>\n",
       "      <td>-0.522373</td>\n",
       "      <td>-0.006629</td>\n",
       "      <td>-0.759095</td>\n",
       "      <td>0.281007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213038</td>\n",
       "      <td>-0.014411</td>\n",
       "      <td>-0.255918</td>\n",
       "      <td>-0.120095</td>\n",
       "      <td>-0.074851</td>\n",
       "      <td>0.438338</td>\n",
       "      <td>-0.304013</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>0.035895</td>\n",
       "      <td>-0.009754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261110</th>\n",
       "      <td>-1.225787</td>\n",
       "      <td>-0.035555</td>\n",
       "      <td>1.673703</td>\n",
       "      <td>0.582572</td>\n",
       "      <td>-0.171355</td>\n",
       "      <td>0.438098</td>\n",
       "      <td>0.986569</td>\n",
       "      <td>0.154613</td>\n",
       "      <td>-0.249954</td>\n",
       "      <td>-0.805134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533896</td>\n",
       "      <td>0.372035</td>\n",
       "      <td>0.543561</td>\n",
       "      <td>0.149715</td>\n",
       "      <td>0.675198</td>\n",
       "      <td>0.748660</td>\n",
       "      <td>-0.393897</td>\n",
       "      <td>-0.004364</td>\n",
       "      <td>0.103384</td>\n",
       "      <td>0.606311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "88005   0.065203  0.560112  1.125474  0.540353 -0.536414 -0.320562  0.302465   \n",
       "167431 -0.873838  0.474974 -1.415775 -0.484232  1.016395 -0.760008  1.889260   \n",
       "59392   1.279947 -0.846119  0.426752 -0.840060 -0.996439  0.027175 -0.972928   \n",
       "20131   1.225637 -0.649268 -0.119134 -0.513312 -0.797736 -0.561421 -0.522373   \n",
       "261110 -1.225787 -0.035555  1.673703  0.582572 -0.171355  0.438098  0.986569   \n",
       "\n",
       "              V8        V9       V10     ...           V20       V21  \\\n",
       "88005   0.057764  0.060539  0.017287     ...     -0.108628 -0.051882   \n",
       "167431  0.044478 -1.530959 -0.754338     ...      0.271137  0.537073   \n",
       "59392   0.208130 -0.651566  0.785956     ...      0.039822  0.406552   \n",
       "20131  -0.006629 -0.759095  0.281007     ...      0.213038 -0.014411   \n",
       "261110  0.154613 -0.249954 -0.805134     ...      0.533896  0.372035   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "88005  -0.043935  0.279408  0.550676 -1.400744  0.109110  0.047491  0.154411   \n",
       "167431  1.012363 -0.061086  0.792703  1.023667  0.670202 -0.314630 -0.170169   \n",
       "59392   0.999654 -0.170415 -0.302680  0.445933  0.002176  0.015554  0.002923   \n",
       "20131  -0.255918 -0.120095 -0.074851  0.438338 -0.304013 -0.001022  0.035895   \n",
       "261110  0.543561  0.149715  0.675198  0.748660 -0.393897 -0.004364  0.103384   \n",
       "\n",
       "        normAmount  \n",
       "88005    -0.237525  \n",
       "167431    0.422119  \n",
       "59392    -0.192826  \n",
       "20131    -0.009754  \n",
       "261110    0.606311  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts = features['Amount']\n",
    "features = features.drop(['Amount'], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#?????????????????\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try resampling\n",
    "\n",
    "#f1 scorer\n",
    "\n",
    "#k-nn and random forest classifiers\n",
    "\n",
    "#neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    199016\n",
      "1       348\n",
      "Name: Label, dtype: int64\n",
      "0    85299\n",
      "1      144\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = .3, random_state = 331)\n",
    "\n",
    "print y_train.value_counts()\n",
    "print y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Random Forest Classifier:\n",
      "[ 0.99973097  0.9997679   0.99975208] 0.999750317137\n",
      "For K-Nearest Neighbors Classifier:\n",
      "[ 0.99971515  0.9997468   0.99973097] 0.999730973346\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "print \"For Random Forest Classifier:\"\n",
    "rfscores = cross_val_score(rf, features, labels, scoring = f1_scorer)\n",
    "print rfscores, rfscores.mean()\n",
    "\n",
    "print \"For K-Nearest Neighbors Classifier:\"\n",
    "knnscores = cross_val_score(knn, features, labels, scoring = f1_scorer)\n",
    "print knnscores, knnscores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for simple majority vote is  0.999135510488\n"
     ]
    }
   ],
   "source": [
    "#majority vote benchmark\n",
    "majority_vote_predictions = np.zeros(features.shape[0])\n",
    "print \"f1 score for simple majority vote is \" , f1_score(labels, majority_vote_predictions, pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# V this is stupid V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=5, score=0.999547 -   0.5s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=5, score=0.999505 -   0.5s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=5, score=0.999547 -   0.4s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999568 -   0.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999558 -   0.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=17, score=0.999600 -   0.7s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=25, score=0.999484 -   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=25, score=0.999547 -   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=25, score=0.999547 -   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=gini, max_features=17, score=0.999568 -   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=gini, max_features=17, score=0.999505 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=gini, max_features=17, score=0.999537 -   0.1s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=21, score=0.999494 -   0.6s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=21, score=0.999547 -   0.6s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=21, score=0.999579 -   0.6s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=25, score=0.999568 -   0.3s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=25, score=0.999579 -   0.3s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=entropy, max_features=25, score=0.999600 -   0.3s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=entropy, max_features=17, score=0.999568 -   0.5s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=entropy, max_features=17, score=0.999568 -   0.5s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=entropy, max_features=17, score=0.999621 -   0.5s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=9, score=0.999579 -   0.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=9, score=0.999568 -   1.0s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=9, score=0.999600 -   1.1s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=entropy, max_features=13, score=0.999547 -   0.8s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=entropy, max_features=13, score=0.999568 -   0.7s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=entropy, max_features=13, score=0.999600 -   0.6s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=13, score=0.999515 -   0.5s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=13, score=0.999558 -   0.4s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=13, score=0.999579 -   0.4s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=gini, max_features=13, score=0.999494 -   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=gini, max_features=13, score=0.999537 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=6, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=10, min_samples_split=6, criterion=gini, max_features=13, score=0.999568 -   0.0s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=25, score=0.999526 -   1.3s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=25, score=0.999568 -   1.5s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=25, score=0.999610 -   2.4s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=21, score=0.999568 -   1.1s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=21, score=0.999568 -   0.9s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=21, score=0.999579 -   0.7s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=25, score=0.999547 -   1.1s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=25, score=0.999558 -   1.2s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=25, score=0.999568 -   1.2s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=9, score=0.999537 -   0.5s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=9, score=0.999526 -   0.6s\n",
      "[CV] n_estimators=40, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=40, min_samples_split=6, criterion=gini, max_features=9, score=0.999558 -   0.4s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=entropy, max_features=17, score=0.999568 -   0.8s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=entropy, max_features=17, score=0.999568 -   0.6s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=entropy, max_features=17, score=0.999589 -   0.7s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=17, score=0.999526 -   0.4s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=17, score=0.999558 -   0.4s\n",
      "[CV] n_estimators=40, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=40, min_samples_split=4, criterion=gini, max_features=17, score=0.999589 -   0.4s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=entropy, max_features=5, score=0.999526 -   0.7s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=entropy, max_features=5, score=0.999515 -   0.7s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=entropy, max_features=5, score=0.999568 -   0.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999537 -   0.9s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999568 -   0.9s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999600 -   0.9s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=25, score=0.999526 -   1.0s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=25, score=0.999526 -   1.1s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=25 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=25, score=0.999610 -   1.1s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=17, score=0.999568 -   0.9s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=17, score=0.999568 -   0.9s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=17, score=0.999558 -   0.9s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=5, score=0.999547 -   1.3s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=5, score=0.999537 -   1.3s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=gini, max_features=5, score=0.999547 -   1.3s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=5, score=0.999515 -   0.9s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=5, score=0.999526 -   0.9s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=5, score=0.999558 -   0.8s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=9, score=0.999558 -   0.6s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=9, score=0.999547 -   0.6s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=9, score=0.999579 -   0.7s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=entropy, max_features=5, score=0.999505 -   1.0s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=entropy, max_features=5, score=0.999558 -   1.0s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=entropy, max_features=5, score=0.999600 -   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 430.2min finished\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators' : np.arange(10, 110, 15),\n",
    "                'min_samples_split': np.arange(2, 8, 2),\n",
    "                'max_features': np.arange(5, 29, 4),\n",
    "                'criterion': ['gini', 'entropy']}\n",
    "\n",
    "'''\n",
    "knn_params = {'n_neighbors': np.arange(3, 10),\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'p': np.arange(1, 3)} \n",
    "''' \n",
    "\n",
    "rf_tune = RandomizedSearchCV(rf, rf_params, n_iter = 25, verbose = 3)\n",
    "#knn_tune = RandomizedSearchCV(knn, knn_params, n_iter = 20, verbose = 3)\n",
    "\n",
    "rf_tune = rf_tune.fit(features, labels)\n",
    "#knn_tune = knn_tune.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features=17, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=55, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False) \n",
      "f1 score: 0.999585684341\n"
     ]
    }
   ],
   "source": [
    "print rf_tune.best_estimator_ , '\\nf1 score:' , rf_tune.best_score_\n",
    "#print knn_tune.best_estimator_ + '\\nf1 score:' + knn_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.36807225, -0.80535119, -0.20908081, ..., -0.30532203,\n",
       "         1.06654398,  0.06716665],\n",
       "       [-0.27124035,  0.55946789, -0.54494174, ..., -0.28095942,\n",
       "         1.33383085,  0.24786847],\n",
       "       [ 0.80936887,  0.61689589,  0.74584106, ...,  0.5148158 ,\n",
       "        -0.30145523,  0.41496762],\n",
       "       ..., \n",
       "       [ 0.42375799,  0.36419524,  2.19072329, ...,  1.21963372,\n",
       "         1.59622618,  0.7538579 ],\n",
       "       [-0.07676136, -0.50647389,  0.07786699, ..., -1.50003717,\n",
       "        -0.04110927,  0.41677922],\n",
       "       [ 0.4690207 , -1.22739193, -0.56715424, ..., -2.73527709,\n",
       "         0.98445114,  1.40371771]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft = rf_tune.best_estimator_\n",
    "rft.fit_transform(X_train, y_train)\n",
    "\n",
    "rfu = RandomForestClassifier()\n",
    "rfu.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 testing score for tuned random forest is  0.999759709776\n",
      "f1 testing score for random forest is  0.99976556914\n"
     ]
    }
   ],
   "source": [
    "# Check performances of tuned and untuned models\n",
    "print \"f1 testing score for tuned random forest is \", f1_score(y_test, rft.predict(X_test), pos_label = 0)\n",
    "\n",
    "print \"f1 testing score for random forest is \" , f1_score(y_test, rfu.predict(X_test), pos_label = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops, looks like I could have chosen a better selection of hyper parameters for the randomized search cross validation optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85293,     6],\n",
       "       [   34,   110]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rfu.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now let's try a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot or nah? idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a model\n",
    "\n",
    "#add regularization\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim = X_train.shape[1], activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(8, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "sgd = SGD(lr = .1, momentum = .8, decay = .001)\n",
    "\n",
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159491 samples, validate on 39873 samples\n",
      "Epoch 1/200\n",
      "1s - loss: 0.0703 - fmeasure: 4.7590e-05 - val_loss: 0.0144 - val_fmeasure: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1s - loss: 0.0120 - fmeasure: 0.0000e+00 - val_loss: 0.0109 - val_fmeasure: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1s - loss: 0.0090 - fmeasure: 0.0000e+00 - val_loss: 0.0077 - val_fmeasure: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1s - loss: 0.0065 - fmeasure: 0.1996 - val_loss: 0.0058 - val_fmeasure: 0.5606\n",
      "Epoch 5/200\n",
      "1s - loss: 0.0053 - fmeasure: 0.5088 - val_loss: 0.0052 - val_fmeasure: 0.6576\n",
      "Epoch 6/200\n",
      "1s - loss: 0.0047 - fmeasure: 0.6381 - val_loss: 0.0049 - val_fmeasure: 0.6917\n",
      "Epoch 7/200\n",
      "1s - loss: 0.0043 - fmeasure: 0.6551 - val_loss: 0.0048 - val_fmeasure: 0.6917\n",
      "Epoch 8/200\n",
      "1s - loss: 0.0041 - fmeasure: 0.6708 - val_loss: 0.0047 - val_fmeasure: 0.7123\n",
      "Epoch 9/200\n",
      "1s - loss: 0.0041 - fmeasure: 0.6819 - val_loss: 0.0046 - val_fmeasure: 0.7123\n",
      "Epoch 10/200\n",
      "1s - loss: 0.0040 - fmeasure: 0.7354 - val_loss: 0.0046 - val_fmeasure: 0.7123\n",
      "Epoch 11/200\n",
      "1s - loss: 0.0038 - fmeasure: 0.7401 - val_loss: 0.0045 - val_fmeasure: 0.7171\n",
      "Epoch 12/200\n",
      "1s - loss: 0.0038 - fmeasure: 0.7124 - val_loss: 0.0045 - val_fmeasure: 0.7171\n",
      "Epoch 13/200\n",
      "1s - loss: 0.0037 - fmeasure: 0.6927 - val_loss: 0.0045 - val_fmeasure: 0.7171\n",
      "Epoch 14/200\n",
      "1s - loss: 0.0037 - fmeasure: 0.7085 - val_loss: 0.0045 - val_fmeasure: 0.7171\n",
      "Epoch 15/200\n",
      "1s - loss: 0.0035 - fmeasure: 0.7491 - val_loss: 0.0045 - val_fmeasure: 0.7171\n",
      "Epoch 16/200\n",
      "1s - loss: 0.0036 - fmeasure: 0.7582 - val_loss: 0.0045 - val_fmeasure: 0.7171\n",
      "Epoch 17/200\n",
      "1s - loss: 0.0035 - fmeasure: 0.7241 - val_loss: 0.0045 - val_fmeasure: 0.7171\n",
      "Epoch 18/200\n",
      "1s - loss: 0.0035 - fmeasure: 0.7171 - val_loss: 0.0045 - val_fmeasure: 0.7312\n",
      "Epoch 19/200\n",
      "1s - loss: 0.0034 - fmeasure: 0.7515 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 20/200\n",
      "1s - loss: 0.0034 - fmeasure: 0.7649 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 21/200\n",
      "1s - loss: 0.0036 - fmeasure: 0.7080 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 22/200\n",
      "1s - loss: 0.0035 - fmeasure: 0.7943 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 23/200\n",
      "1s - loss: 0.0035 - fmeasure: 0.7278 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 24/200\n",
      "1s - loss: 0.0034 - fmeasure: 0.7758 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 25/200\n",
      "1s - loss: 0.0034 - fmeasure: 0.7565 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 26/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7135 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 27/200\n",
      "1s - loss: 0.0034 - fmeasure: 0.7606 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 28/200\n",
      "1s - loss: 0.0034 - fmeasure: 0.6980 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 29/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7484 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 30/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7629 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 31/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7356 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 32/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7710 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 33/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7691 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 34/200\n",
      "1s - loss: 0.0034 - fmeasure: 0.7492 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 35/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7340 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 36/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7194 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 37/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7368 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 38/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7287 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 39/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7469 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 40/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7553 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 41/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7096 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 42/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7618 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 43/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7331 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 44/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7661 - val_loss: 0.0044 - val_fmeasure: 0.7312\n",
      "Epoch 45/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7430 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 46/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7691 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 47/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7488 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 48/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7312 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 49/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7586 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 50/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7592 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 51/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7500 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 52/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7821 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 53/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7567 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 54/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7117 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 55/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7753 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 56/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7493 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 57/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7218 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 58/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7729 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 59/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7630 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 60/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7659 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 61/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7345 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 62/200\n",
      "2s - loss: 0.0031 - fmeasure: 0.7712 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 63/200\n",
      "2s - loss: 0.0031 - fmeasure: 0.7499 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 64/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7371 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 65/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7334 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 66/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7655 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 67/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7488 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 68/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7339 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 69/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7714 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 70/200\n",
      "2s - loss: 0.0031 - fmeasure: 0.7608 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 71/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7398 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 72/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7466 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 73/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7849 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 74/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7477 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 75/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7758 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 76/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7789 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 77/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7795 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 78/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7382 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 79/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7760 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 80/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7566 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 81/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7161 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 82/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7824 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 83/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7449 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 84/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7642 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 85/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7470 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 86/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7630 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 87/200\n",
      "1s - loss: 0.0033 - fmeasure: 0.7178 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 88/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7741 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 89/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7725 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 90/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7700 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 91/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7563 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 92/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7396 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 93/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7472 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 94/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7259 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 95/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7466 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 96/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7614 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 97/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7747 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 98/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7406 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 99/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7365 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 100/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7521 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 101/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7709 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 102/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7864 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 103/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7559 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 104/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7312 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 105/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7866 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 106/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7869 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 107/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7728 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 108/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7546 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 109/200\n",
      "1s - loss: 0.0032 - fmeasure: 0.7067 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 110/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7639 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 111/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7699 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 112/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7669 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 113/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7689 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 114/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7023 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 115/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7615 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 116/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7259 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 117/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7249 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 118/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7484 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 119/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7733 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 120/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7384 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 121/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7676 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 122/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7408 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 123/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7059 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 124/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7711 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 125/200\n",
      "2s - loss: 0.0031 - fmeasure: 0.7380 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 126/200\n",
      "2s - loss: 0.0030 - fmeasure: 0.7468 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 127/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7519 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 128/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7467 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 129/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7460 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 130/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7683 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 131/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7588 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 132/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7579 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 133/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7638 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 134/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7618 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 135/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7103 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 136/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7281 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 137/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7843 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 138/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7880 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 139/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7477 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 140/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7494 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 141/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7889 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 142/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7844 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 143/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7474 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 144/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7726 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 145/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7936 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 146/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7710 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 147/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7202 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 148/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7550 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 149/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7748 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 150/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7501 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 151/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7724 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 152/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7794 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 153/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7689 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 154/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7519 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 155/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7829 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 156/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7267 - val_loss: 0.0043 - val_fmeasure: 0.7312\n",
      "Epoch 157/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7967 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 158/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7580 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 159/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7727 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 160/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7151 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 161/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7764 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 162/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7585 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 163/200\n",
      "2s - loss: 0.0030 - fmeasure: 0.7797 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 164/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.8060 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 165/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7413 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 166/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7768 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 167/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7851 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 168/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7388 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 169/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7731 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 170/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7758 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 171/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7593 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 172/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7870 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 173/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7624 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 174/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7652 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 175/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7566 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 176/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7396 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 177/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7938 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 178/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7457 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 179/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7277 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 180/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7475 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 181/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7383 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 182/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7447 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 183/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7779 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 184/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7651 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 185/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7879 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 186/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7362 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 187/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7267 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 188/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7714 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 189/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7694 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 190/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7603 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 191/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7644 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 192/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7930 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 193/200\n",
      "1s - loss: 0.0031 - fmeasure: 0.7163 - val_loss: 0.0042 - val_fmeasure: 0.7366\n",
      "Epoch 194/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7457 - val_loss: 0.0042 - val_fmeasure: 0.7366\n",
      "Epoch 195/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7741 - val_loss: 0.0042 - val_fmeasure: 0.7366\n",
      "Epoch 196/200\n",
      "1s - loss: 0.0030 - fmeasure: 0.7385 - val_loss: 0.0042 - val_fmeasure: 0.7366\n",
      "Epoch 197/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7177 - val_loss: 0.0042 - val_fmeasure: 0.7366\n",
      "Epoch 198/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7350 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 199/200\n",
      "1s - loss: 0.0029 - fmeasure: 0.7772 - val_loss: 0.0042 - val_fmeasure: 0.7312\n",
      "Epoch 200/200\n",
      "2s - loss: 0.0029 - fmeasure: 0.7601 - val_loss: 0.0042 - val_fmeasure: 0.7366\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train.values, y_train.values, nb_epoch = 200, batch_size = 2000, verbose = 2, validation_split = .20,\n",
    "                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
